{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_Training-zh.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "usZ7DeHwQY5f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 计算机视觉纳米学位项目\n",
        "\n",
        "## 实战项目：图像标注\n",
        "\n",
        "---\n",
        "\n",
        "在这个notebook中，你要做的是训练你的CNN-RNN模型。\n",
        "\n",
        "我们欢迎并鼓励你在搜索好的模型时尝试多种不同的架构和超参数。\n",
        "\n",
        "这样的话，很有可能会使项目变得非常凌乱！所以，在提交项目之前，请确保清理以下内容：\n",
        "- 你在这个notebook上写的代码。该notebook应描述如何训练单个CNN-RNN架构，使其与你最终选择的超参数相对应。此外，你的notebook应便于审阅专家通过运行此notebook中的代码来复制结果。\n",
        "- **Step 2**中代码单元格的输出。这个输出显示的应该是从零开始训练模型时获得的输出。\n",
        "\n",
        "我们将会对这个notebook**进行评分**。\n",
        "\n",
        "你可以通过点击以下链接导航到该notebook：\n",
        "- [Step 1](#step1): 训练设置\n",
        "- [Step 2](#step2): 训练你的模型\n",
        "- [Step 3](#step3): (可选）验证你的模型\n",
        "\n",
        "<a id='step1'></a>\n",
        "## Step 1: 训练设置\n",
        "\n",
        "在该notebook的此步骤中，你需要通过定义超参数并设置训练过程中重要的其他选项来自定义对CNN-RNN模型的训练。在下面的**Step 2**中训练模型时，会使用到现在设置的值。\n",
        "\n",
        "请注意，你只可以修改以`TODO`语句开头的代码块。**对于所以不在`TODO`语句之前的代码块，不能做任何修改。**\n",
        "\n",
        "### 任务 #1\n",
        "\n",
        "首先，请设置以下变量：\n",
        "- `batch_size` - 每个训练批次的批次大小。它是指用于在每个训练步骤中修改模型权重的图像标注对的数量。\n",
        "- `vocab_threshold` - 单词阈值最小值。请注意，阈值越大，词汇量越小，而阈值越小，则表示将包括较少的词汇，词汇量则越大。\n",
        "- `vocab_from_file` - 一个布尔值，用于决定是否从文件加载词汇表。\n",
        "- `embed_size` - the dimensionality of the image and word embeddings.  图像和单词嵌入的维度。\n",
        "- `hidden_size` - RNN解码器隐藏状态下的特征数。\n",
        "- `num_epochs` - 训练模型的epoch数。我们建议你设置为`num_epochs=3`，但可以根据需要随意增加或减少此数字。 [这篇论文](https://arxiv.org/pdf/1502.03044.pdf) 在一个最先进的GPU上对一个标注生成模型训练了3天，但很快你就会发现，其实在几个小时内就可以得到合理的结果！（_但是，如果你想让你的模型与当前的研究一较高下，则需要更长时间的训练。_)\n",
        "- `save_every` - 确定保存模型权重的频率。我们建议你设置为`save_every=1`，便于在每个epoch后保存模型权重。这样，在第`i`个epoch之后，编码器和解码器权重将在`models/`文件夹中分别保存为`encoder-i.pkl`和`decoder-i.pkl`。\n",
        "- `print_every` - 确定在训练时将批次损失输出到Jupyter notebook的频率。请注意，训练时，你**将不会**看到损失函数的单调减少，这一点非常好并且完全可以预料到！我们建议你将其保持在默认值`100` ，从而避免让这个notebook运行变慢，但之后随时都可以进行更改。\n",
        "- `log_file` - 包含每个步骤中训练期间的损失与复杂度演变过程的的文本文件的名称。\n",
        "\n",
        "对于上述某些值，如果你不确定从哪里开始设置，可以仔细阅读 [这篇文章](https://arxiv.org/pdf/1502.03044.pdf) 与 [这篇文章](https://arxiv.org/pdf/1411.4555.pdf) ，获得有用的指导！为了避免在该notebook上花费太长时间，我们建议你查阅这些研究论文，从中可以获得有关哪些超参数可能最有效的初始猜测。然后，训练单个模型，然后继续下一个notebook（**3_Inference.ipynb**）。如果你对模型的效果不满意，可以返回此notebook调整超参数和/或**model.py**中的体系结构，并重新训练模型。\n",
        "\n",
        "### 问题1\n",
        "\n",
        "**问题:** 详细描述你的CNN-RNN架构。对于这种架构，任务1中变量的值，你是如何选择的？如果你查阅了某一篇详细说明关于成功实现图像标注生成模型的研究论文，请提供该参考论文。\n",
        "\n",
        "**答案:** \n",
        "\n",
        "\n",
        "### （可选）任务 #2\n",
        "\n",
        "请注意，我们为你推荐了一个用于预处理训练图像的图像转换`transform_train`，但同时，也欢迎并鼓励你根据需要进行修改。修改此转换时，请牢记：\n",
        "- 数据集中的图像具有不同的高度和宽度\n",
        "- 如果使用预先训练的模型，则必须执行相应的相应归一化。\n",
        "\n",
        "### 问题2\n",
        "\n",
        "**问题:** 你是如何在`transform_train`中选择转换方式的？如果你将转换保留为其提供的值，为什么你的任务它非常适合你的CNN架构？\n",
        "\n",
        "**答案:** \n",
        "\n",
        "### 任务 #3\n",
        "\n",
        "接下来，你需要定义一个包含模型的可学习参数的Python列表。 例如，如果你决定使解码器中的所有权重都是可训练的，但只想在编码器的嵌入层中训练权重，那么，就应该将`params`设置为："
      ]
    },
    {
      "metadata": {
        "id": "5arTd0DKQY5h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "GQaHv8h2QY5i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 问题3\n",
        "\n",
        "**问题:** 你是如何选择该架构的可训练参数的？ 为什么你认为这是一个不错的选择？\n",
        "\n",
        "**答案:** \n",
        "\n",
        "### 任务 #4\n",
        "\n",
        "最后，选择一个 [优化程序](http://pytorch.org/docs/master/optim.html#torch.optim.Optimizer)。\n",
        "\n",
        "### 问题4\n",
        "\n",
        "**问题:** 你是如何选择用于训练模型的优化程序的？\n",
        "\n",
        "**答案:**"
      ]
    },
    {
      "metadata": {
        "id": "7bUFbiXDQY5m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import sys\n",
        "sys.path.append('/opt/cocoapi/PythonAPI')\n",
        "from pycocotools.coco import COCO\n",
        "from data_loader import get_loader\n",
        "from model import EncoderCNN, DecoderRNN\n",
        "import math\n",
        "import torch.optim as optim\n",
        "\n",
        "# batch_size = 64          # batch size\n",
        "# vocab_threshold = 10        # minimum word count threshold\n",
        "# vocab_from_file = True    # if True, load existing vocab file\n",
        "# embed_size = 256           # dimensionality of image and word embeddings\n",
        "# hidden_size = 64          # number of features in hidden state of the RNN decoder\n",
        "# num_epochs = 3             # number of training epochs\n",
        "# save_every = 1             # determines frequency of saving model weights\n",
        "# print_every = 100          # determines window for printing average loss\n",
        "# log_file = 'training_log.txt'       # name of file with saved training loss and perplexity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XJkESHfFTv0i",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "## TODO #1: Select appropriate values for the Python variables below.\n",
        "\n",
        "#@title Hyper parameter\n",
        "batch_size = 64 #@param {type:\"integer\"}\n",
        "vocab_threshold = 10 #@param {type:\"integer\"}\n",
        "vocab_from_file = True #@param {type:\"boolean\"}\n",
        "embed_size = 256 #@param {type:\"integer\"}\n",
        "hidden_size = 64 #@param {type:\"integer\"}\n",
        "num_layers = 1 #@param {type:\"integer\"}\n",
        "num_epochs = 3 #@param {type:\"integer\"}\n",
        "save_every = 1 #@param {type:\"integer\"}\n",
        "print_every = 100 #@param {type:\"integer\"}\n",
        "log_file = 'training_log.txt' #@param {type:\"string\"}\n",
        "save_root_path = \"/content/gdrive/My Drive/AI-projects/cv-project1-image-caption/\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "t1vv13oCQY5r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8364
        },
        "outputId": "99f02660-84aa-4188-eebe-985aca80dfea"
      },
      "cell_type": "code",
      "source": [
        "# (Optional) TODO #2: Amend the image transform below.\n",
        "transform_train = transforms.Compose([ \n",
        "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
        "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
        "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
        "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
        "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
        "                         (0.229, 0.224, 0.225))])\n",
        "\n",
        "# Build data loader.\n",
        "data_loader = get_loader(transform=transform_train,\n",
        "                         mode='train',\n",
        "                         batch_size=batch_size,\n",
        "                         vocab_threshold=vocab_threshold,\n",
        "                         vocab_from_file=vocab_from_file)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary successfully loaded from vocab.pkl file!\n",
            "loading annotations into memory...\n",
            "Done (t=0.93s)\n",
            "creating index...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/414113 [00:00<?, ?it/s]\u001b[A\n",
            "  0%|          | 742/414113 [00:00<00:55, 7414.21it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "index created!\n",
            "Obtaining caption lengths...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 1536/414113 [00:00<00:54, 7562.26it/s]\u001b[A\n",
            "  1%|          | 2360/414113 [00:00<00:53, 7750.99it/s]\u001b[A\n",
            "  1%|          | 3213/414113 [00:00<00:51, 7967.89it/s]\u001b[A\n",
            "  1%|          | 3965/414113 [00:00<00:52, 7827.86it/s]\u001b[A\n",
            "  1%|          | 4852/414113 [00:00<00:50, 8111.64it/s]\u001b[A\n",
            "  1%|▏         | 5629/414113 [00:00<00:51, 8005.26it/s]\u001b[A\n",
            "  2%|▏         | 6517/414113 [00:00<00:49, 8247.29it/s]\u001b[A\n",
            "  2%|▏         | 7377/414113 [00:00<00:48, 8349.33it/s]\u001b[A\n",
            "  2%|▏         | 8266/414113 [00:01<00:47, 8502.59it/s]\u001b[A\n",
            "  2%|▏         | 9143/414113 [00:01<00:47, 8578.86it/s]\u001b[A\n",
            "  2%|▏         | 9989/414113 [00:01<00:48, 8397.61it/s]\u001b[A\n",
            "  3%|▎         | 10862/414113 [00:01<00:47, 8492.50it/s]\u001b[A\n",
            "  3%|▎         | 11733/414113 [00:01<00:47, 8556.55it/s]\u001b[A\n",
            "  3%|▎         | 12617/414113 [00:01<00:46, 8637.54it/s]\u001b[A\n",
            "  3%|▎         | 13507/414113 [00:01<00:45, 8711.57it/s]\u001b[A\n",
            "  3%|▎         | 14390/414113 [00:01<00:45, 8744.42it/s]\u001b[A\n",
            "  4%|▎         | 15280/414113 [00:01<00:45, 8788.98it/s]\u001b[A\n",
            "  4%|▍         | 16177/414113 [00:01<00:45, 8841.28it/s]\u001b[A\n",
            "  4%|▍         | 17061/414113 [00:02<00:45, 8808.25it/s]\u001b[A\n",
            "  4%|▍         | 17963/414113 [00:02<00:44, 8869.28it/s]\u001b[A\n",
            "  5%|▍         | 18850/414113 [00:02<00:45, 8643.13it/s]\u001b[A\n",
            "  5%|▍         | 19738/414113 [00:02<00:45, 8712.37it/s]\u001b[A\n",
            "  5%|▍         | 20617/414113 [00:02<00:45, 8733.26it/s]\u001b[A\n",
            "  5%|▌         | 21504/414113 [00:02<00:44, 8772.41it/s]\u001b[A\n",
            "  5%|▌         | 22399/414113 [00:02<00:44, 8822.99it/s]\u001b[A\n",
            "  6%|▌         | 23298/414113 [00:02<00:44, 8870.47it/s]\u001b[A\n",
            "  6%|▌         | 24188/414113 [00:02<00:43, 8877.67it/s]\u001b[A\n",
            "  6%|▌         | 25077/414113 [00:02<00:43, 8867.95it/s]\u001b[A\n",
            "  6%|▋         | 25972/414113 [00:03<00:43, 8890.25it/s]\u001b[A\n",
            "  6%|▋         | 26866/414113 [00:03<00:43, 8903.55it/s]\u001b[A\n",
            "  7%|▋         | 27757/414113 [00:03<00:44, 8721.15it/s]\u001b[A\n",
            "  7%|▋         | 28631/414113 [00:03<00:45, 8539.02it/s]\u001b[A\n",
            "  7%|▋         | 29491/414113 [00:03<00:44, 8557.13it/s]\u001b[A\n",
            "  7%|▋         | 30348/414113 [00:04<01:54, 3362.23it/s]\u001b[A\n",
            "  8%|▊         | 31259/414113 [00:04<01:32, 4146.75it/s]\u001b[A\n",
            "  8%|▊         | 32125/414113 [00:04<01:17, 4914.90it/s]\u001b[A\n",
            "  8%|▊         | 32976/414113 [00:04<01:07, 5627.91it/s]\u001b[A\n",
            "  8%|▊         | 33833/414113 [00:04<01:00, 6273.11it/s]\u001b[A\n",
            "  8%|▊         | 34714/414113 [00:04<00:55, 6864.65it/s]\u001b[A\n",
            "  9%|▊         | 35589/414113 [00:04<00:51, 7338.98it/s]\u001b[A\n",
            "  9%|▉         | 36459/414113 [00:04<00:49, 7698.51it/s]\u001b[A\n",
            "  9%|▉         | 37356/414113 [00:04<00:46, 8040.21it/s]\u001b[A\n",
            "  9%|▉         | 38234/414113 [00:04<00:45, 8246.53it/s]\u001b[A\n",
            "  9%|▉         | 39104/414113 [00:05<00:44, 8375.49it/s]\u001b[A\n",
            " 10%|▉         | 39999/414113 [00:05<00:43, 8538.52it/s]\u001b[A\n",
            " 10%|▉         | 40899/414113 [00:05<00:43, 8669.62it/s]\u001b[A\n",
            " 10%|█         | 41783/414113 [00:05<00:43, 8574.65it/s]\u001b[A\n",
            " 10%|█         | 42653/414113 [00:05<00:43, 8581.41it/s]\u001b[A\n",
            " 11%|█         | 43520/414113 [00:05<00:43, 8601.91it/s]\u001b[A\n",
            " 11%|█         | 44386/414113 [00:05<00:42, 8618.75it/s]\u001b[A\n",
            " 11%|█         | 45252/414113 [00:05<00:42, 8622.92it/s]\u001b[A\n",
            " 11%|█         | 46138/414113 [00:05<00:42, 8691.15it/s]\u001b[A\n",
            " 11%|█▏        | 47028/414113 [00:05<00:41, 8752.12it/s]\u001b[A\n",
            " 12%|█▏        | 47905/414113 [00:06<00:41, 8726.91it/s]\u001b[A\n",
            " 12%|█▏        | 48779/414113 [00:06<00:42, 8675.29it/s]\u001b[A\n",
            " 12%|█▏        | 49675/414113 [00:06<00:41, 8756.66it/s]\u001b[A\n",
            " 12%|█▏        | 50552/414113 [00:06<00:42, 8614.54it/s]\u001b[A\n",
            " 12%|█▏        | 51450/414113 [00:06<00:41, 8718.70it/s]\u001b[A\n",
            " 13%|█▎        | 52323/414113 [00:06<00:41, 8683.04it/s]\u001b[A\n",
            " 13%|█▎        | 53222/414113 [00:06<00:41, 8771.57it/s]\u001b[A\n",
            " 13%|█▎        | 54121/414113 [00:06<00:40, 8835.09it/s]\u001b[A\n",
            " 13%|█▎        | 55006/414113 [00:06<00:42, 8526.97it/s]\u001b[A\n",
            " 13%|█▎        | 55887/414113 [00:06<00:41, 8607.87it/s]\u001b[A\n",
            " 14%|█▎        | 56773/414113 [00:07<00:41, 8681.51it/s]\u001b[A\n",
            " 14%|█▍        | 57667/414113 [00:07<00:40, 8757.44it/s]\u001b[A\n",
            " 14%|█▍        | 58556/414113 [00:07<00:40, 8794.47it/s]\u001b[A\n",
            " 14%|█▍        | 59437/414113 [00:07<00:41, 8571.79it/s]\u001b[A\n",
            " 15%|█▍        | 60341/414113 [00:07<00:40, 8705.99it/s]\u001b[A\n",
            " 15%|█▍        | 61236/414113 [00:07<00:40, 8776.09it/s]\u001b[A\n",
            " 15%|█▍        | 62116/414113 [00:07<00:40, 8742.75it/s]\u001b[A\n",
            " 15%|█▌        | 62992/414113 [00:07<00:40, 8738.78it/s]\u001b[A\n",
            " 15%|█▌        | 63867/414113 [00:07<00:40, 8696.78it/s]\u001b[A\n",
            " 16%|█▌        | 64738/414113 [00:07<00:40, 8667.26it/s]\u001b[A\n",
            " 16%|█▌        | 65606/414113 [00:08<00:40, 8571.66it/s]\u001b[A\n",
            " 16%|█▌        | 66491/414113 [00:08<00:40, 8652.16it/s]\u001b[A\n",
            " 16%|█▋        | 67382/414113 [00:08<00:39, 8726.41it/s]\u001b[A\n",
            " 16%|█▋        | 68256/414113 [00:08<00:40, 8571.07it/s]\u001b[A\n",
            " 17%|█▋        | 69138/414113 [00:08<00:39, 8641.07it/s]\u001b[A\n",
            " 17%|█▋        | 70040/414113 [00:08<00:39, 8750.40it/s]\u001b[A\n",
            " 17%|█▋        | 70929/414113 [00:08<00:39, 8789.75it/s]\u001b[A\n",
            " 17%|█▋        | 71809/414113 [00:08<00:39, 8749.59it/s]\u001b[A\n",
            " 18%|█▊        | 72685/414113 [00:08<00:39, 8747.36it/s]\u001b[A\n",
            " 18%|█▊        | 73587/414113 [00:09<00:38, 8826.61it/s]\u001b[A\n",
            " 18%|█▊        | 74471/414113 [00:09<00:38, 8813.97it/s]\u001b[A\n",
            " 18%|█▊        | 75353/414113 [00:09<00:38, 8766.43it/s]\u001b[A\n",
            " 18%|█▊        | 76234/414113 [00:09<00:38, 8779.26it/s]\u001b[A\n",
            " 19%|█▊        | 77113/414113 [00:09<00:39, 8616.43it/s]\u001b[A\n",
            " 19%|█▉        | 78018/414113 [00:09<00:38, 8740.16it/s]\u001b[A\n",
            " 19%|█▉        | 78912/414113 [00:09<00:38, 8797.04it/s]\u001b[A\n",
            " 19%|█▉        | 79799/414113 [00:09<00:37, 8816.46it/s]\u001b[A\n",
            " 19%|█▉        | 80704/414113 [00:09<00:37, 8883.65it/s]\u001b[A\n",
            " 20%|█▉        | 81593/414113 [00:09<00:37, 8872.90it/s]\u001b[A\n",
            " 20%|█▉        | 82495/414113 [00:10<00:37, 8914.44it/s]\u001b[A\n",
            " 20%|██        | 83393/414113 [00:10<00:37, 8933.11it/s]\u001b[A\n",
            " 20%|██        | 84287/414113 [00:10<00:36, 8922.08it/s]\u001b[A\n",
            " 21%|██        | 85180/414113 [00:10<00:37, 8872.70it/s]\u001b[A\n",
            " 21%|██        | 86068/414113 [00:10<00:37, 8675.48it/s]\u001b[A\n",
            " 21%|██        | 86984/414113 [00:10<00:37, 8813.37it/s]\u001b[A\n",
            " 21%|██        | 87867/414113 [00:10<00:37, 8798.41it/s]\u001b[A\n",
            " 21%|██▏       | 88766/414113 [00:10<00:36, 8852.25it/s]\u001b[A\n",
            " 22%|██▏       | 89669/414113 [00:10<00:36, 8902.61it/s]\u001b[A\n",
            " 22%|██▏       | 90560/414113 [00:10<00:36, 8888.22it/s]\u001b[A\n",
            " 22%|██▏       | 91450/414113 [00:11<00:36, 8763.11it/s]\u001b[A\n",
            " 22%|██▏       | 92327/414113 [00:11<00:36, 8717.15it/s]\u001b[A\n",
            " 23%|██▎       | 93217/414113 [00:11<00:36, 8768.88it/s]\u001b[A\n",
            " 23%|██▎       | 94116/414113 [00:11<00:36, 8831.81it/s]\u001b[A\n",
            " 23%|██▎       | 95000/414113 [00:11<00:36, 8668.22it/s]\u001b[A\n",
            " 23%|██▎       | 95903/414113 [00:11<00:36, 8772.45it/s]\u001b[A\n",
            " 23%|██▎       | 96785/414113 [00:11<00:36, 8784.40it/s]\u001b[A\n",
            " 24%|██▎       | 97684/414113 [00:11<00:35, 8843.48it/s]\u001b[A\n",
            " 24%|██▍       | 98587/414113 [00:11<00:35, 8895.83it/s]\u001b[A\n",
            " 24%|██▍       | 99478/414113 [00:11<00:35, 8877.00it/s]\u001b[A\n",
            " 24%|██▍       | 100373/414113 [00:12<00:35, 8897.52it/s]\u001b[A\n",
            " 24%|██▍       | 101264/414113 [00:12<00:35, 8872.42it/s]\u001b[A\n",
            " 25%|██▍       | 102154/414113 [00:12<00:35, 8878.74it/s]\u001b[A\n",
            " 25%|██▍       | 103065/414113 [00:12<00:34, 8944.48it/s]\u001b[A\n",
            " 25%|██▌       | 103960/414113 [00:12<00:35, 8830.97it/s]\u001b[A\n",
            " 25%|██▌       | 104844/414113 [00:12<00:35, 8718.31it/s]\u001b[A\n",
            " 26%|██▌       | 105717/414113 [00:12<00:35, 8640.50it/s]\u001b[A\n",
            " 26%|██▌       | 106600/414113 [00:12<00:35, 8694.60it/s]\u001b[A\n",
            " 26%|██▌       | 107490/414113 [00:12<00:35, 8752.89it/s]\u001b[A\n",
            " 26%|██▌       | 108366/414113 [00:12<00:35, 8730.10it/s]\u001b[A\n",
            " 26%|██▋       | 109257/414113 [00:13<00:34, 8781.33it/s]\u001b[A\n",
            " 27%|██▋       | 110145/414113 [00:13<00:34, 8809.88it/s]\u001b[A\n",
            " 27%|██▋       | 111027/414113 [00:13<00:34, 8717.38it/s]\u001b[A\n",
            " 27%|██▋       | 111918/414113 [00:13<00:34, 8773.23it/s]\u001b[A\n",
            " 27%|██▋       | 112796/414113 [00:13<00:34, 8764.31it/s]\u001b[A\n",
            " 27%|██▋       | 113673/414113 [00:13<00:34, 8610.10it/s]\u001b[A\n",
            " 28%|██▊       | 114572/414113 [00:13<00:34, 8716.49it/s]\u001b[A\n",
            " 28%|██▊       | 115445/414113 [00:13<00:34, 8704.10it/s]\u001b[A\n",
            " 28%|██▊       | 116345/414113 [00:13<00:33, 8789.89it/s]\u001b[A\n",
            " 28%|██▊       | 117246/414113 [00:13<00:33, 8852.60it/s]\u001b[A\n",
            " 29%|██▊       | 118140/414113 [00:14<00:33, 8877.96it/s]\u001b[A\n",
            " 29%|██▊       | 119029/414113 [00:14<00:33, 8881.18it/s]\u001b[A\n",
            " 29%|██▉       | 119931/414113 [00:14<00:32, 8920.21it/s]\u001b[A\n",
            " 29%|██▉       | 120836/414113 [00:14<00:32, 8957.18it/s]\u001b[A\n",
            " 29%|██▉       | 121732/414113 [00:14<00:32, 8925.80it/s]\u001b[A\n",
            " 30%|██▉       | 122625/414113 [00:14<00:33, 8733.20it/s]\u001b[A\n",
            " 30%|██▉       | 123506/414113 [00:14<00:33, 8754.31it/s]\u001b[A\n",
            " 30%|███       | 124425/414113 [00:14<00:32, 8878.87it/s]\u001b[A\n",
            " 30%|███       | 125332/414113 [00:14<00:32, 8933.46it/s]\u001b[A\n",
            " 30%|███       | 126231/414113 [00:14<00:32, 8949.70it/s]\u001b[A\n",
            " 31%|███       | 127134/414113 [00:15<00:31, 8972.33it/s]\u001b[A\n",
            " 31%|███       | 128032/414113 [00:15<00:31, 8964.29it/s]\u001b[A\n",
            " 31%|███       | 128929/414113 [00:15<00:32, 8868.66it/s]\u001b[A\n",
            " 31%|███▏      | 129822/414113 [00:15<00:31, 8885.02it/s]\u001b[A\n",
            " 32%|███▏      | 130711/414113 [00:15<00:57, 4945.12it/s]\u001b[A\n",
            " 32%|███▏      | 131593/414113 [00:15<00:49, 5695.17it/s]\u001b[A\n",
            " 32%|███▏      | 132493/414113 [00:15<00:44, 6398.89it/s]\u001b[A\n",
            " 32%|███▏      | 133390/414113 [00:16<00:40, 6999.56it/s]\u001b[A\n",
            " 32%|███▏      | 134263/414113 [00:16<00:37, 7440.84it/s]\u001b[A\n",
            " 33%|███▎      | 135155/414113 [00:16<00:35, 7829.54it/s]\u001b[A\n",
            " 33%|███▎      | 136055/414113 [00:16<00:34, 8147.36it/s]\u001b[A\n",
            " 33%|███▎      | 136941/414113 [00:16<00:33, 8348.20it/s]\u001b[A\n",
            " 33%|███▎      | 137832/414113 [00:16<00:32, 8502.66it/s]\u001b[A\n",
            " 33%|███▎      | 138711/414113 [00:16<00:32, 8477.50it/s]\u001b[A\n",
            " 34%|███▎      | 139587/414113 [00:16<00:32, 8558.99it/s]\u001b[A\n",
            " 34%|███▍      | 140457/414113 [00:16<00:31, 8585.49it/s]\u001b[A\n",
            " 34%|███▍      | 141330/414113 [00:16<00:31, 8628.18it/s]\u001b[A\n",
            " 34%|███▍      | 142220/414113 [00:17<00:31, 8706.21it/s]\u001b[A\n",
            " 35%|███▍      | 143104/414113 [00:17<00:30, 8743.55it/s]\u001b[A\n",
            " 35%|███▍      | 143985/414113 [00:17<00:30, 8762.90it/s]\u001b[A\n",
            " 35%|███▍      | 144891/414113 [00:17<00:30, 8849.17it/s]\u001b[A\n",
            " 35%|███▌      | 145797/414113 [00:17<00:30, 8909.56it/s]\u001b[A\n",
            " 35%|███▌      | 146690/414113 [00:17<00:30, 8836.26it/s]\u001b[A\n",
            " 36%|███▌      | 147575/414113 [00:17<00:30, 8669.61it/s]\u001b[A\n",
            " 36%|███▌      | 148473/414113 [00:17<00:30, 8759.17it/s]\u001b[A\n",
            " 36%|███▌      | 149357/414113 [00:17<00:30, 8781.07it/s]\u001b[A\n",
            " 36%|███▋      | 150253/414113 [00:17<00:29, 8832.22it/s]\u001b[A\n",
            " 37%|███▋      | 151159/414113 [00:18<00:29, 8897.10it/s]\u001b[A\n",
            " 37%|███▋      | 152063/414113 [00:18<00:29, 8939.28it/s]\u001b[A\n",
            " 37%|███▋      | 152972/414113 [00:18<00:29, 8982.84it/s]\u001b[A\n",
            " 37%|███▋      | 153871/414113 [00:18<00:29, 8972.94it/s]\u001b[A\n",
            " 37%|███▋      | 154781/414113 [00:18<00:28, 9010.20it/s]\u001b[A\n",
            " 38%|███▊      | 155683/414113 [00:18<00:29, 8905.09it/s]\u001b[A\n",
            " 38%|███▊      | 156574/414113 [00:18<00:29, 8758.08it/s]\u001b[A\n",
            " 38%|███▊      | 157465/414113 [00:18<00:29, 8802.57it/s]\u001b[A\n",
            " 38%|███▊      | 158358/414113 [00:18<00:28, 8839.03it/s]\u001b[A\n",
            " 38%|███▊      | 159257/414113 [00:18<00:28, 8883.33it/s]\u001b[A\n",
            " 39%|███▊      | 160159/414113 [00:19<00:28, 8922.18it/s]\u001b[A\n",
            " 39%|███▉      | 161058/414113 [00:19<00:28, 8939.57it/s]\u001b[A\n",
            " 39%|███▉      | 161963/414113 [00:19<00:28, 8971.35it/s]\u001b[A\n",
            " 39%|███▉      | 162895/414113 [00:19<00:27, 9071.06it/s]\u001b[A\n",
            " 40%|███▉      | 163803/414113 [00:19<00:28, 8938.06it/s]\u001b[A\n",
            " 40%|███▉      | 164700/414113 [00:19<00:27, 8946.29it/s]\u001b[A\n",
            " 40%|███▉      | 165596/414113 [00:19<00:28, 8700.88it/s]\u001b[A\n",
            " 40%|████      | 166468/414113 [00:19<00:28, 8705.32it/s]\u001b[A\n",
            " 40%|████      | 167365/414113 [00:19<00:28, 8782.66it/s]\u001b[A\n",
            " 41%|████      | 168255/414113 [00:19<00:27, 8817.31it/s]\u001b[A\n",
            " 41%|████      | 169150/414113 [00:20<00:27, 8854.33it/s]\u001b[A\n",
            " 41%|████      | 170048/414113 [00:20<00:27, 8891.08it/s]\u001b[A\n",
            " 41%|████▏     | 170938/414113 [00:20<00:27, 8877.52it/s]\u001b[A\n",
            " 41%|████▏     | 171828/414113 [00:20<00:27, 8882.31it/s]\u001b[A\n",
            " 42%|████▏     | 172717/414113 [00:20<00:27, 8851.29it/s]\u001b[A\n",
            " 42%|████▏     | 173603/414113 [00:20<00:27, 8769.83it/s]\u001b[A\n",
            " 42%|████▏     | 174481/414113 [00:20<00:28, 8407.80it/s]\u001b[A\n",
            " 42%|████▏     | 175372/414113 [00:20<00:27, 8551.32it/s]\u001b[A\n",
            " 43%|████▎     | 176246/414113 [00:20<00:27, 8606.07it/s]\u001b[A\n",
            " 43%|████▎     | 177120/414113 [00:21<00:27, 8644.29it/s]\u001b[A\n",
            " 43%|████▎     | 178010/414113 [00:21<00:27, 8718.26it/s]\u001b[A\n",
            " 43%|████▎     | 178897/414113 [00:21<00:26, 8762.31it/s]\u001b[A\n",
            " 43%|████▎     | 179785/414113 [00:21<00:26, 8796.86it/s]\u001b[A\n",
            " 44%|████▎     | 180666/414113 [00:21<00:26, 8787.80it/s]\u001b[A\n",
            " 44%|████▍     | 181546/414113 [00:21<00:26, 8790.91it/s]\u001b[A\n",
            " 44%|████▍     | 182429/414113 [00:21<00:26, 8800.41it/s]\u001b[A\n",
            " 44%|████▍     | 183310/414113 [00:21<00:26, 8654.34it/s]\u001b[A\n",
            " 44%|████▍     | 184207/414113 [00:21<00:26, 8746.01it/s]\u001b[A\n",
            " 45%|████▍     | 185099/414113 [00:21<00:26, 8795.48it/s]\u001b[A\n",
            " 45%|████▍     | 186005/414113 [00:22<00:25, 8871.46it/s]\u001b[A\n",
            " 45%|████▌     | 186893/414113 [00:22<00:25, 8839.76it/s]\u001b[A\n",
            " 45%|████▌     | 187801/414113 [00:22<00:25, 8909.99it/s]\u001b[A\n",
            " 46%|████▌     | 188693/414113 [00:22<00:25, 8866.88it/s]\u001b[A\n",
            " 46%|████▌     | 189581/414113 [00:22<00:25, 8803.49it/s]\u001b[A\n",
            " 46%|████▌     | 190488/414113 [00:22<00:25, 8880.59it/s]\u001b[A\n",
            " 46%|████▌     | 191399/414113 [00:22<00:24, 8946.86it/s]\u001b[A\n",
            " 46%|████▋     | 192295/414113 [00:22<00:25, 8728.76it/s]\u001b[A\n",
            " 47%|████▋     | 193190/414113 [00:22<00:25, 8792.34it/s]\u001b[A\n",
            " 47%|████▋     | 194071/414113 [00:22<00:25, 8796.20it/s]\u001b[A\n",
            " 47%|████▋     | 194969/414113 [00:23<00:24, 8849.53it/s]\u001b[A\n",
            " 47%|████▋     | 195862/414113 [00:23<00:24, 8870.96it/s]\u001b[A\n",
            " 48%|████▊     | 196750/414113 [00:23<00:24, 8822.48it/s]\u001b[A\n",
            " 48%|████▊     | 197651/414113 [00:23<00:24, 8876.93it/s]\u001b[A\n",
            " 48%|████▊     | 198558/414113 [00:23<00:24, 8932.20it/s]\u001b[A\n",
            " 48%|████▊     | 199452/414113 [00:23<00:24, 8931.42it/s]\u001b[A\n",
            " 48%|████▊     | 200346/414113 [00:23<00:24, 8900.97it/s]\u001b[A\n",
            " 49%|████▊     | 201237/414113 [00:23<00:24, 8649.96it/s]\u001b[A\n",
            " 49%|████▉     | 202147/414113 [00:23<00:24, 8777.85it/s]\u001b[A\n",
            " 49%|████▉     | 203046/414113 [00:23<00:23, 8839.36it/s]\u001b[A\n",
            " 49%|████▉     | 203935/414113 [00:24<00:23, 8853.87it/s]\u001b[A\n",
            " 49%|████▉     | 204822/414113 [00:24<00:23, 8766.38it/s]\u001b[A\n",
            " 50%|████▉     | 205700/414113 [00:24<00:23, 8727.07it/s]\u001b[A\n",
            " 50%|████▉     | 206574/414113 [00:24<00:23, 8680.64it/s]\u001b[A\n",
            " 50%|█████     | 207463/414113 [00:24<00:23, 8742.23it/s]\u001b[A\n",
            " 50%|█████     | 208350/414113 [00:24<00:23, 8776.98it/s]\u001b[A\n",
            " 51%|█████     | 209256/414113 [00:24<00:23, 8857.91it/s]\u001b[A\n",
            " 51%|█████     | 210143/414113 [00:24<00:23, 8804.60it/s]\u001b[A\n",
            " 51%|█████     | 211024/414113 [00:24<00:23, 8755.77it/s]\u001b[A\n",
            " 51%|█████     | 211932/414113 [00:24<00:22, 8848.48it/s]\u001b[A\n",
            " 51%|█████▏    | 212821/414113 [00:25<00:22, 8860.17it/s]\u001b[A\n",
            " 52%|█████▏    | 213708/414113 [00:25<00:22, 8839.06it/s]\u001b[A\n",
            " 52%|█████▏    | 214593/414113 [00:25<00:22, 8777.17it/s]\u001b[A\n",
            " 52%|█████▏    | 215471/414113 [00:25<00:23, 8592.65it/s]\u001b[A\n",
            " 52%|█████▏    | 216332/414113 [00:25<00:23, 8554.39it/s]\u001b[A\n",
            " 52%|█████▏    | 217215/414113 [00:25<00:22, 8633.15it/s]\u001b[A\n",
            " 53%|█████▎    | 218080/414113 [00:25<00:22, 8625.86it/s]\u001b[A\n",
            " 53%|█████▎    | 218944/414113 [00:25<00:22, 8605.55it/s]\u001b[A\n",
            " 53%|█████▎    | 219805/414113 [00:25<00:22, 8451.14it/s]\u001b[A\n",
            " 53%|█████▎    | 220685/414113 [00:25<00:22, 8551.32it/s]\u001b[A\n",
            " 54%|█████▎    | 221574/414113 [00:26<00:22, 8649.28it/s]\u001b[A\n",
            " 54%|█████▎    | 222495/414113 [00:26<00:21, 8808.93it/s]\u001b[A\n",
            " 54%|█████▍    | 223413/414113 [00:26<00:21, 8916.89it/s]\u001b[A\n",
            " 54%|█████▍    | 224307/414113 [00:26<00:21, 8921.71it/s]\u001b[A\n",
            " 54%|█████▍    | 225201/414113 [00:26<00:21, 8904.06it/s]\u001b[A\n",
            " 55%|█████▍    | 226095/414113 [00:26<00:21, 8913.96it/s]\u001b[A\n",
            " 55%|█████▍    | 226987/414113 [00:26<00:20, 8913.46it/s]\u001b[A\n",
            " 55%|█████▌    | 227904/414113 [00:26<00:20, 8986.69it/s]\u001b[A\n",
            " 55%|█████▌    | 228804/414113 [00:26<00:20, 8854.05it/s]\u001b[A\n",
            " 55%|█████▌    | 229691/414113 [00:26<00:20, 8839.43it/s]\u001b[A\n",
            " 56%|█████▌    | 230576/414113 [00:27<00:21, 8401.37it/s]\u001b[A\n",
            " 56%|█████▌    | 231422/414113 [00:27<00:21, 8406.58it/s]\u001b[A\n",
            " 56%|█████▌    | 232304/414113 [00:27<00:21, 8526.43it/s]\u001b[A\n",
            " 56%|█████▋    | 233189/414113 [00:27<00:20, 8619.25it/s]\u001b[A\n",
            " 57%|█████▋    | 234080/414113 [00:27<00:20, 8702.21it/s]\u001b[A\n",
            " 57%|█████▋    | 234975/414113 [00:27<00:20, 8773.49it/s]\u001b[A\n",
            " 57%|█████▋    | 235854/414113 [00:27<00:20, 8766.80it/s]\u001b[A\n",
            " 57%|█████▋    | 236732/414113 [00:27<00:20, 8740.88it/s]\u001b[A\n",
            " 57%|█████▋    | 237607/414113 [00:27<00:20, 8485.20it/s]\u001b[A\n",
            " 58%|█████▊    | 238459/414113 [00:28<00:20, 8494.28it/s]\u001b[A\n",
            " 58%|█████▊    | 239348/414113 [00:28<00:20, 8607.76it/s]\u001b[A\n",
            " 58%|█████▊    | 240238/414113 [00:28<00:20, 8692.40it/s]\u001b[A\n",
            " 58%|█████▊    | 241127/414113 [00:28<00:19, 8749.47it/s]\u001b[A\n",
            " 58%|█████▊    | 242023/414113 [00:28<00:19, 8810.10it/s]\u001b[A\n",
            " 59%|█████▊    | 242905/414113 [00:28<00:19, 8782.42it/s]\u001b[A\n",
            " 59%|█████▉    | 243795/414113 [00:28<00:19, 8814.96it/s]\u001b[A\n",
            " 59%|█████▉    | 244690/414113 [00:28<00:19, 8854.31it/s]\u001b[A\n",
            " 59%|█████▉    | 245576/414113 [00:28<00:19, 8838.67it/s]\u001b[A\n",
            " 60%|█████▉    | 246461/414113 [00:28<00:19, 8615.38it/s]\u001b[A\n",
            " 60%|█████▉    | 247360/414113 [00:29<00:19, 8723.66it/s]\u001b[A\n",
            " 60%|█████▉    | 248234/414113 [00:29<00:19, 8718.01it/s]\u001b[A\n",
            " 60%|██████    | 249116/414113 [00:29<00:18, 8747.13it/s]\u001b[A\n",
            " 60%|██████    | 249992/414113 [00:29<00:19, 8510.93it/s]\u001b[A\n",
            " 61%|██████    | 250845/414113 [00:29<00:19, 8382.49it/s]\u001b[A\n",
            " 61%|██████    | 251749/414113 [00:29<00:18, 8567.56it/s]\u001b[A\n",
            " 61%|██████    | 252628/414113 [00:29<00:18, 8630.26it/s]\u001b[A\n",
            " 61%|██████    | 253516/414113 [00:29<00:18, 8703.49it/s]\u001b[A\n",
            " 61%|██████▏   | 254402/414113 [00:29<00:18, 8749.64it/s]\u001b[A\n",
            " 62%|██████▏   | 255278/414113 [00:29<00:18, 8527.13it/s]\u001b[A\n",
            " 62%|██████▏   | 256133/414113 [00:30<00:35, 4511.69it/s]\u001b[A\n",
            " 62%|██████▏   | 257026/414113 [00:30<00:29, 5297.28it/s]\u001b[A\n",
            " 62%|██████▏   | 257940/414113 [00:30<00:25, 6061.72it/s]\u001b[A\n",
            " 63%|██████▎   | 258830/414113 [00:30<00:23, 6700.25it/s]\u001b[A\n",
            " 63%|██████▎   | 259712/414113 [00:30<00:21, 7219.11it/s]\u001b[A\n",
            " 63%|██████▎   | 260596/414113 [00:30<00:20, 7638.52it/s]\u001b[A\n",
            " 63%|██████▎   | 261447/414113 [00:30<00:19, 7785.47it/s]\u001b[A\n",
            " 63%|██████▎   | 262316/414113 [00:31<00:18, 8034.97it/s]\u001b[A\n",
            " 64%|██████▎   | 263176/414113 [00:31<00:18, 8195.62it/s]\u001b[A\n",
            " 64%|██████▍   | 264041/414113 [00:31<00:18, 8325.21it/s]\u001b[A\n",
            " 64%|██████▍   | 264906/414113 [00:31<00:17, 8419.31it/s]\u001b[A\n",
            " 64%|██████▍   | 265797/414113 [00:31<00:17, 8559.33it/s]\u001b[A\n",
            " 64%|██████▍   | 266705/414113 [00:31<00:16, 8706.74it/s]\u001b[A\n",
            " 65%|██████▍   | 267591/414113 [00:31<00:16, 8749.86it/s]\u001b[A\n",
            " 65%|██████▍   | 268473/414113 [00:31<00:16, 8672.43it/s]\u001b[A\n",
            " 65%|██████▌   | 269358/414113 [00:31<00:16, 8721.88it/s]\u001b[A\n",
            " 65%|██████▌   | 270234/414113 [00:31<00:16, 8563.91it/s]\u001b[A\n",
            " 65%|██████▌   | 271125/414113 [00:32<00:16, 8664.49it/s]\u001b[A\n",
            " 66%|██████▌   | 272010/414113 [00:32<00:16, 8717.81it/s]\u001b[A\n",
            " 66%|██████▌   | 272908/414113 [00:32<00:16, 8794.82it/s]\u001b[A\n",
            " 66%|██████▌   | 273813/414113 [00:32<00:15, 8868.94it/s]\u001b[A\n",
            " 66%|██████▋   | 274708/414113 [00:32<00:15, 8892.62it/s]\u001b[A\n",
            " 67%|██████▋   | 275611/414113 [00:32<00:15, 8930.78it/s]\u001b[A\n",
            " 67%|██████▋   | 276512/414113 [00:32<00:15, 8953.61it/s]\u001b[A\n",
            " 67%|██████▋   | 277408/414113 [00:32<00:15, 8878.45it/s]\u001b[A\n",
            " 67%|██████▋   | 278297/414113 [00:32<00:15, 8839.97it/s]\u001b[A\n",
            " 67%|██████▋   | 279182/414113 [00:32<00:15, 8675.23it/s]\u001b[A\n",
            " 68%|██████▊   | 280055/414113 [00:33<00:15, 8689.48it/s]\u001b[A\n",
            " 68%|██████▊   | 280927/414113 [00:33<00:15, 8697.63it/s]\u001b[A\n",
            " 68%|██████▊   | 281798/414113 [00:33<00:15, 8666.63it/s]\u001b[A\n",
            " 68%|██████▊   | 282692/414113 [00:33<00:15, 8745.17it/s]\u001b[A\n",
            " 68%|██████▊   | 283599/414113 [00:33<00:14, 8840.01it/s]\u001b[A\n",
            " 69%|██████▊   | 284488/414113 [00:33<00:14, 8852.79it/s]\u001b[A\n",
            " 69%|██████▉   | 285393/414113 [00:33<00:14, 8909.23it/s]\u001b[A\n",
            " 69%|██████▉   | 286285/414113 [00:33<00:14, 8850.15it/s]\u001b[A\n",
            " 69%|██████▉   | 287171/414113 [00:33<00:14, 8797.47it/s]\u001b[A\n",
            " 70%|██████▉   | 288052/414113 [00:33<00:14, 8712.81it/s]\u001b[A\n",
            " 70%|██████▉   | 288924/414113 [00:34<00:14, 8651.36it/s]\u001b[A\n",
            " 70%|██████▉   | 289790/414113 [00:34<00:14, 8595.24it/s]\u001b[A\n",
            " 70%|███████   | 290652/414113 [00:34<00:14, 8602.14it/s]\u001b[A\n",
            " 70%|███████   | 291513/414113 [00:34<00:14, 8603.91it/s]\u001b[A\n",
            " 71%|███████   | 292383/414113 [00:34<00:14, 8630.55it/s]\u001b[A\n",
            " 71%|███████   | 293306/414113 [00:34<00:13, 8801.58it/s]\u001b[A\n",
            " 71%|███████   | 294213/414113 [00:34<00:13, 8878.36it/s]\u001b[A\n",
            " 71%|███████▏  | 295102/414113 [00:34<00:13, 8816.03it/s]\u001b[A\n",
            " 71%|███████▏  | 296008/414113 [00:34<00:13, 8883.59it/s]\u001b[A\n",
            " 72%|███████▏  | 296898/414113 [00:34<00:13, 8885.14it/s]\u001b[A\n",
            " 72%|███████▏  | 297787/414113 [00:35<00:13, 8580.40it/s]\u001b[A\n",
            " 72%|███████▏  | 298683/414113 [00:35<00:13, 8690.09it/s]\u001b[A\n",
            " 72%|███████▏  | 299590/414113 [00:35<00:13, 8800.56it/s]\u001b[A\n",
            " 73%|███████▎  | 300483/414113 [00:35<00:12, 8836.24it/s]\u001b[A\n",
            " 73%|███████▎  | 301368/414113 [00:35<00:12, 8837.05it/s]\u001b[A\n",
            " 73%|███████▎  | 302265/414113 [00:35<00:12, 8876.39it/s]\u001b[A\n",
            " 73%|███████▎  | 303173/414113 [00:35<00:12, 8935.86it/s]\u001b[A\n",
            " 73%|███████▎  | 304068/414113 [00:35<00:12, 8698.24it/s]\u001b[A\n",
            " 74%|███████▎  | 304940/414113 [00:35<00:13, 8317.48it/s]\u001b[A\n",
            " 74%|███████▍  | 305777/414113 [00:36<00:13, 7922.12it/s]\u001b[A\n",
            " 74%|███████▍  | 306577/414113 [00:36<00:13, 7685.84it/s]\u001b[A\n",
            " 74%|███████▍  | 307353/414113 [00:36<00:14, 7546.20it/s]\u001b[A\n",
            " 74%|███████▍  | 308113/414113 [00:36<00:14, 7512.70it/s]\u001b[A\n",
            " 75%|███████▍  | 308868/414113 [00:36<00:14, 7456.93it/s]\u001b[A\n",
            " 75%|███████▍  | 309624/414113 [00:36<00:13, 7487.21it/s]\u001b[A\n",
            " 75%|███████▍  | 310376/414113 [00:36<00:13, 7495.32it/s]\u001b[A\n",
            " 75%|███████▌  | 311127/414113 [00:36<00:13, 7475.35it/s]\u001b[A\n",
            " 75%|███████▌  | 311876/414113 [00:36<00:13, 7432.38it/s]\u001b[A\n",
            " 75%|███████▌  | 312635/414113 [00:36<00:13, 7477.25it/s]\u001b[A\n",
            " 76%|███████▌  | 313384/414113 [00:37<00:13, 7384.24it/s]\u001b[A\n",
            " 76%|███████▌  | 314124/414113 [00:37<00:13, 7266.83it/s]\u001b[A\n",
            " 76%|███████▌  | 314873/414113 [00:37<00:13, 7332.27it/s]\u001b[A\n",
            " 76%|███████▌  | 315626/414113 [00:37<00:13, 7388.30it/s]\u001b[A\n",
            " 76%|███████▋  | 316387/414113 [00:37<00:13, 7451.78it/s]\u001b[A\n",
            " 77%|███████▋  | 317154/414113 [00:37<00:12, 7513.36it/s]\u001b[A\n",
            " 77%|███████▋  | 317908/414113 [00:37<00:12, 7520.01it/s]\u001b[A\n",
            " 77%|███████▋  | 318675/414113 [00:37<00:12, 7562.12it/s]\u001b[A\n",
            " 77%|███████▋  | 319433/414113 [00:37<00:12, 7566.16it/s]\u001b[A\n",
            " 77%|███████▋  | 320197/414113 [00:37<00:12, 7587.63it/s]\u001b[A\n",
            " 78%|███████▊  | 320956/414113 [00:38<00:12, 7533.86it/s]\u001b[A\n",
            " 78%|███████▊  | 321710/414113 [00:38<00:13, 7096.58it/s]\u001b[A\n",
            " 78%|███████▊  | 322426/414113 [00:38<00:12, 7096.26it/s]\u001b[A\n",
            " 78%|███████▊  | 323170/414113 [00:38<00:12, 7193.68it/s]\u001b[A\n",
            " 78%|███████▊  | 323899/414113 [00:38<00:12, 7220.91it/s]\u001b[A\n",
            " 78%|███████▊  | 324639/414113 [00:38<00:12, 7272.61it/s]\u001b[A\n",
            " 79%|███████▊  | 325391/414113 [00:38<00:12, 7342.72it/s]\u001b[A\n",
            " 79%|███████▉  | 326139/414113 [00:38<00:11, 7381.38it/s]\u001b[A\n",
            " 79%|███████▉  | 326883/414113 [00:38<00:11, 7394.61it/s]\u001b[A\n",
            " 79%|███████▉  | 327636/414113 [00:39<00:11, 7434.36it/s]\u001b[A\n",
            " 79%|███████▉  | 328380/414113 [00:39<00:11, 7423.37it/s]\u001b[A\n",
            " 79%|███████▉  | 329123/414113 [00:39<00:11, 7220.05it/s]\u001b[A\n",
            " 80%|███████▉  | 329847/414113 [00:39<00:11, 7193.37it/s]\u001b[A\n",
            " 80%|███████▉  | 330568/414113 [00:39<00:11, 7189.28it/s]\u001b[A\n",
            " 80%|███████▉  | 331288/414113 [00:39<00:11, 7142.54it/s]\u001b[A\n",
            " 80%|████████  | 332005/414113 [00:39<00:11, 7150.71it/s]\u001b[A\n",
            " 80%|████████  | 332741/414113 [00:39<00:11, 7211.51it/s]\u001b[A\n",
            " 81%|████████  | 333476/414113 [00:39<00:11, 7250.46it/s]\u001b[A\n",
            " 81%|████████  | 334218/414113 [00:39<00:10, 7299.88it/s]\u001b[A\n",
            " 81%|████████  | 334949/414113 [00:40<00:10, 7270.03it/s]\u001b[A\n",
            " 81%|████████  | 335677/414113 [00:40<00:10, 7258.40it/s]\u001b[A\n",
            " 81%|████████  | 336404/414113 [00:40<00:10, 7100.93it/s]\u001b[A\n",
            " 81%|████████▏ | 337128/414113 [00:40<00:10, 7141.02it/s]\u001b[A\n",
            " 82%|████████▏ | 337855/414113 [00:40<00:10, 7178.32it/s]\u001b[A\n",
            " 82%|████████▏ | 338574/414113 [00:40<00:10, 7152.15it/s]\u001b[A\n",
            " 82%|████████▏ | 339301/414113 [00:40<00:10, 7184.91it/s]\u001b[A\n",
            " 82%|████████▏ | 340036/414113 [00:40<00:10, 7230.67it/s]\u001b[A\n",
            " 82%|████████▏ | 340779/414113 [00:40<00:10, 7288.53it/s]\u001b[A\n",
            " 82%|████████▏ | 341509/414113 [00:40<00:10, 7248.08it/s]\u001b[A\n",
            " 83%|████████▎ | 342253/414113 [00:41<00:09, 7303.86it/s]\u001b[A\n",
            " 83%|████████▎ | 342997/414113 [00:41<00:09, 7343.77it/s]\u001b[A\n",
            " 83%|████████▎ | 343732/414113 [00:41<00:09, 7190.16it/s]\u001b[A\n",
            " 83%|████████▎ | 344480/414113 [00:41<00:09, 7273.31it/s]\u001b[A\n",
            " 83%|████████▎ | 345227/414113 [00:41<00:09, 7329.22it/s]\u001b[A\n",
            " 84%|████████▎ | 345966/414113 [00:41<00:09, 7345.26it/s]\u001b[A\n",
            " 84%|████████▎ | 346709/414113 [00:41<00:09, 7369.62it/s]\u001b[A\n",
            " 84%|████████▍ | 347447/414113 [00:41<00:09, 7360.40it/s]\u001b[A\n",
            " 84%|████████▍ | 348186/414113 [00:41<00:08, 7367.86it/s]\u001b[A\n",
            " 84%|████████▍ | 348923/414113 [00:41<00:08, 7359.44it/s]\u001b[A\n",
            " 84%|████████▍ | 349669/414113 [00:42<00:08, 7388.80it/s]\u001b[A\n",
            " 85%|████████▍ | 350416/414113 [00:42<00:08, 7411.17it/s]\u001b[A\n",
            " 85%|████████▍ | 351158/414113 [00:42<00:08, 7267.05it/s]\u001b[A\n",
            " 85%|████████▍ | 351888/414113 [00:42<00:08, 7276.87it/s]\u001b[A\n",
            " 85%|████████▌ | 352632/414113 [00:42<00:08, 7323.89it/s]\u001b[A\n",
            " 85%|████████▌ | 353373/414113 [00:42<00:08, 7348.48it/s]\u001b[A\n",
            " 86%|████████▌ | 354127/414113 [00:42<00:08, 7403.67it/s]\u001b[A\n",
            " 86%|████████▌ | 354875/414113 [00:42<00:07, 7423.87it/s]\u001b[A\n",
            " 86%|████████▌ | 355618/414113 [00:42<00:07, 7423.90it/s]\u001b[A\n",
            " 86%|████████▌ | 356365/414113 [00:42<00:07, 7436.68it/s]\u001b[A\n",
            " 86%|████████▌ | 357109/414113 [00:43<00:07, 7380.61it/s]\u001b[A\n",
            " 86%|████████▋ | 357853/414113 [00:43<00:07, 7397.73it/s]\u001b[A\n",
            " 87%|████████▋ | 358593/414113 [00:43<00:07, 7231.00it/s]\u001b[A\n",
            " 87%|████████▋ | 359330/414113 [00:43<00:07, 7269.63it/s]\u001b[A\n",
            " 87%|████████▋ | 360063/414113 [00:43<00:07, 7286.57it/s]\u001b[A\n",
            " 87%|████████▋ | 360793/414113 [00:43<00:07, 7254.80it/s]\u001b[A\n",
            " 87%|████████▋ | 361525/414113 [00:43<00:07, 7271.76it/s]\u001b[A\n",
            " 87%|████████▋ | 362253/414113 [00:43<00:07, 7259.07it/s]\u001b[A\n",
            " 88%|████████▊ | 362991/414113 [00:43<00:07, 7294.09it/s]\u001b[A\n",
            " 88%|████████▊ | 363721/414113 [00:43<00:07, 7166.67it/s]\u001b[A\n",
            " 88%|████████▊ | 364460/414113 [00:44<00:06, 7231.69it/s]\u001b[A\n",
            " 88%|████████▊ | 365201/414113 [00:44<00:06, 7281.77it/s]\u001b[A\n",
            " 88%|████████▊ | 365930/414113 [00:44<00:06, 7251.49it/s]\u001b[A\n",
            " 89%|████████▊ | 366656/414113 [00:44<00:06, 7138.52it/s]\u001b[A\n",
            " 89%|████████▊ | 367389/414113 [00:44<00:06, 7192.85it/s]\u001b[A\n",
            " 89%|████████▉ | 368132/414113 [00:44<00:06, 7260.45it/s]\u001b[A\n",
            " 89%|████████▉ | 368863/414113 [00:44<00:06, 7274.54it/s]\u001b[A\n",
            " 89%|████████▉ | 369616/414113 [00:44<00:06, 7347.73it/s]\u001b[A\n",
            " 89%|████████▉ | 370352/414113 [00:44<00:05, 7337.99it/s]\u001b[A\n",
            " 90%|████████▉ | 371087/414113 [00:44<00:05, 7316.01it/s]\u001b[A\n",
            " 90%|████████▉ | 371819/414113 [00:45<00:05, 7313.57it/s]\u001b[A\n",
            " 90%|████████▉ | 372551/414113 [00:45<00:05, 7287.77it/s]\u001b[A\n",
            " 90%|█████████ | 373280/414113 [00:45<00:05, 7263.34it/s]\u001b[A\n",
            " 90%|█████████ | 374007/414113 [00:45<00:05, 7107.83it/s]\u001b[A\n",
            " 90%|█████████ | 374755/414113 [00:45<00:05, 7213.79it/s]\u001b[A\n",
            " 91%|█████████ | 375488/414113 [00:45<00:05, 7247.30it/s]\u001b[A\n",
            " 91%|█████████ | 376242/414113 [00:45<00:05, 7330.91it/s]\u001b[A\n",
            " 91%|█████████ | 376995/414113 [00:45<00:05, 7387.46it/s]\u001b[A\n",
            " 91%|█████████▏| 377881/414113 [00:45<00:04, 7774.56it/s]\u001b[A\n",
            " 91%|█████████▏| 378789/414113 [00:45<00:04, 8123.69it/s]\u001b[A\n",
            " 92%|█████████▏| 379664/414113 [00:46<00:04, 8300.01it/s]\u001b[A\n",
            " 92%|█████████▏| 380545/414113 [00:46<00:03, 8444.94it/s]\u001b[A\n",
            " 92%|█████████▏| 381436/414113 [00:46<00:03, 8578.00it/s]\u001b[A\n",
            " 92%|█████████▏| 382298/414113 [00:46<00:03, 8463.07it/s]\u001b[A\n",
            " 93%|█████████▎| 383206/414113 [00:46<00:03, 8637.26it/s]\u001b[A\n",
            " 93%|█████████▎| 384100/414113 [00:46<00:03, 8724.17it/s]\u001b[A\n",
            " 93%|█████████▎| 384986/414113 [00:46<00:03, 8763.86it/s]\u001b[A\n",
            " 93%|█████████▎| 385871/414113 [00:46<00:03, 8787.70it/s]\u001b[A\n",
            " 93%|█████████▎| 386769/414113 [00:46<00:03, 8843.26it/s]\u001b[A\n",
            " 94%|█████████▎| 387666/414113 [00:46<00:02, 8880.21it/s]\u001b[A\n",
            " 94%|█████████▍| 388555/414113 [00:47<00:02, 8837.95it/s]\u001b[A\n",
            " 94%|█████████▍| 389459/414113 [00:47<00:02, 8897.30it/s]\u001b[A\n",
            " 94%|█████████▍| 390350/414113 [00:47<00:02, 8894.59it/s]\u001b[A\n",
            " 94%|█████████▍| 391240/414113 [00:47<00:02, 8650.21it/s]\u001b[A\n",
            " 95%|█████████▍| 392129/414113 [00:47<00:02, 8719.52it/s]\u001b[A\n",
            " 95%|█████████▍| 393024/414113 [00:47<00:02, 8786.65it/s]\u001b[A\n",
            " 95%|█████████▌| 393910/414113 [00:47<00:02, 8806.58it/s]\u001b[A\n",
            " 95%|█████████▌| 394797/414113 [00:47<00:02, 8824.30it/s]\u001b[A\n",
            " 96%|█████████▌| 395680/414113 [00:47<00:02, 8722.92it/s]\u001b[A\n",
            " 96%|█████████▌| 396579/414113 [00:48<00:01, 8800.34it/s]\u001b[A\n",
            " 96%|█████████▌| 397462/414113 [00:48<00:01, 8808.34it/s]\u001b[A\n",
            " 96%|█████████▌| 398344/414113 [00:48<00:01, 8794.11it/s]\u001b[A\n",
            " 96%|█████████▋| 399225/414113 [00:48<00:01, 8797.88it/s]\u001b[A\n",
            " 97%|█████████▋| 400106/414113 [00:48<00:01, 8602.54it/s]\u001b[A\n",
            " 97%|█████████▋| 400968/414113 [00:48<00:01, 8603.26it/s]\u001b[A\n",
            " 97%|█████████▋| 401864/414113 [00:48<00:01, 8707.15it/s]\u001b[A\n",
            " 97%|█████████▋| 402762/414113 [00:48<00:01, 8785.09it/s]\u001b[A\n",
            " 97%|█████████▋| 403642/414113 [00:48<00:01, 8783.01it/s]\u001b[A\n",
            " 98%|█████████▊| 404537/414113 [00:48<00:01, 8830.85it/s]\u001b[A\n",
            " 98%|█████████▊| 405421/414113 [00:49<00:00, 8822.90it/s]\u001b[A\n",
            " 98%|█████████▊| 406309/414113 [00:49<00:00, 8839.44it/s]\u001b[A\n",
            " 98%|█████████▊| 407207/414113 [00:49<00:00, 8879.90it/s]\u001b[A\n",
            " 99%|█████████▊| 408096/414113 [00:49<00:00, 8865.45it/s]\u001b[A\n",
            " 99%|█████████▉| 408983/414113 [00:49<00:00, 8681.38it/s]\u001b[A\n",
            " 99%|█████████▉| 409860/414113 [00:49<00:00, 8707.70it/s]\u001b[A\n",
            " 99%|█████████▉| 410764/414113 [00:49<00:00, 8803.32it/s]\u001b[A\n",
            " 99%|█████████▉| 411657/414113 [00:49<00:00, 8840.59it/s]\u001b[A\n",
            "100%|█████████▉| 412564/414113 [00:49<00:00, 8908.00it/s]\u001b[A\n",
            "100%|█████████▉| 413474/414113 [00:49<00:00, 8963.60it/s]\u001b[A\n",
            "100%|██████████| 414113/414113 [00:50<00:00, 8281.75it/s]\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ASPrQrDJQY5w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The size of the vocabulary.\n",
        "vocab_size = len(data_loader.dataset.vocab)\n",
        "\n",
        "# Initialize the encoder and decoder. \n",
        "encoder = EncoderCNN(embed_size)\n",
        "decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "# Move models to GPU if CUDA is available. \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder.to(device)\n",
        "decoder.to(device)\n",
        "\n",
        "# Define the loss function. \n",
        "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
        "# TODO #3: Specify the learnable parameters of the model.\n",
        "params = list(encoder.parameters()) + list(decoder.parameters())\n",
        "\n",
        "# TODO #4: Define the optimizer.\n",
        "optimizer = optim.Adam(params, lr=0.001)\n",
        "\n",
        "# Set the total number of training steps per epoch.\n",
        "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cm4acufWQY5y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<a id='step2'></a>\n",
        "## Step 2: 训练你的模型\n",
        "\n",
        "在**Step 1**中执行代码单元格后，下面的训练过程应该就不会出现问题了。\n",
        "\n",
        "在这里，完全可以将代码单元格保留其原样，无需修改即可训练模型。但是，如果要修改用于训练下面模型的代码，则必须确保审阅专家能够很容易地看明白你的更改内容。换句话说，请务必提供适当的注释来描述代码的工作方式！\n",
        "\n",
        "你可能会发现，使用加载已保存的权重来恢复训练很有用。在这种情况下，请注意包含你要加载的编码器和解码器权重的文件的名称（`encoder_file`和`decoder_file`）。之后，你就可以使用下面的代码行加载权重："
      ]
    },
    {
      "metadata": {
        "id": "GfqjC-p7QY5z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "# Load pre-trained weights before resuming training.\n",
        "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
        "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "MgEswS40QY50",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "在试验参数时，请务必记录大量笔记并记录你在各种训练中使用的设置。特别是，你不希望遇到这样的情况，即已经训练了几个小时的模型，但不记得使用的设置:)。\n",
        "\n",
        "### 关于调整超参数的说明\n",
        "\n",
        "为了弄清楚模型的运行情况，你可以尝试去了解训练过程中训练损失和复杂度是如何演变的。为了做好本项目，我们建议你根据这些信息修改超参数。\n",
        "\n",
        "但是，这样你还是无法知道模型是否过度拟合训练数据，但你要知道的是，过度拟合是训练图像标注模型时常会遇到的问题。\n",
        "\n",
        "对于这个项目，你不必担心过度拟合。**该项目对模型的性能没有严格的要求**，你只需要证明你的模型在生成基于测试数据的标注时学到了**_一些东西_**。现在，我们强烈建议你为我们建议的3个epoch训练你的模型，但不必担心性能；然后，立即转换到下一个notebook（**3_Inference.ipynb**），查看模型对测试数据的执行情况。如果你的模型需要更改，可以回到这个notebook，修改超参数（如有必要的话），并重新训练该模型。\n",
        "\n",
        "也就是说，如果你想在这个项目中有所超越，可以阅读 [本文](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505636)4.3.1节中最小化过度拟合的一些方法。在本notebook的下一个（可选）步骤中，我们提供了一些关于评估验证数据集性能的指导。"
      ]
    },
    {
      "metadata": {
        "id": "oUhSWRWXBszM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.save(decoder.state_dict(), os.path.join(save_root_path, './models', 'decoder-%d.pkl' % 0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "UvgR5zR3QY51",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2159
        },
        "outputId": "4a916130-3e97-403a-8cbe-4e3ecb20b1f1"
      },
      "cell_type": "code",
      "source": [
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import os\n",
        "import requests\n",
        "import time\n",
        "\n",
        "# Open the training log file.\n",
        "f = open(save_root_path + log_file, 'w')\n",
        "\n",
        "# old_time = time.time()\n",
        "# response = requests.request(\"GET\", \n",
        "#                             \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
        "#                             headers={\"Metadata-Flavor\":\"Google\"})\n",
        "\n",
        "history = {'train_loss': []}\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    \n",
        "    for i_step in range(1, total_step+1):\n",
        "        \n",
        "#         if time.time() - old_time > 60:\n",
        "#             old_time = time.time()\n",
        "#             requests.request(\"POST\", \n",
        "#                              \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
        "#                              headers={'Authorization': \"STAR \" + response.text})\n",
        "        \n",
        "        # Randomly sample a caption length, and sample indices with that length.\n",
        "        indices = data_loader.dataset.get_train_indices()\n",
        "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
        "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
        "        data_loader.batch_sampler.sampler = new_sampler\n",
        "        \n",
        "        # Obtain the batch.\n",
        "        images, captions = next(iter(data_loader))\n",
        "\n",
        "        # Move batch of images and captions to GPU if CUDA is available.\n",
        "        images = images.to(device)\n",
        "        captions = captions.to(device)\n",
        "        \n",
        "        # Zero the gradients.\n",
        "        decoder.zero_grad()\n",
        "        encoder.zero_grad()\n",
        "        \n",
        "        # Pass the inputs through the CNN-RNN model.\n",
        "        features = encoder(images)\n",
        "        outputs = decoder(features, captions)\n",
        "        \n",
        "        # Calculate the batch loss.\n",
        "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
        "        \n",
        "        # Backward pass.\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update the parameters in the optimizer.\n",
        "        optimizer.step()\n",
        "            \n",
        "        # Get training statistics.\n",
        "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
        "        \n",
        "        # Print training statistics (on same line).\n",
        "        print('\\r' + stats, end=\"\")\n",
        "        sys.stdout.flush()\n",
        "        \n",
        "        # Print training statistics to file.\n",
        "        f.write(stats + '\\n')\n",
        "        f.flush()\n",
        "        history['train_loss'].append(loss.item())\n",
        "        # Print training statistics (on different line).\n",
        "        if i_step % print_every == 0:\n",
        "            print('\\r' + stats)\n",
        "            \n",
        "    # Save the weights.\n",
        "    if epoch % save_every == 0:\n",
        "        torch.save(decoder.state_dict(), os.path.join(save_root_path, './models', 'decoder-%d.pkl' % epoch))\n",
        "        torch.save(encoder.state_dict(), os.path.join(save_root_path, './models', 'encoder-%d.pkl' % epoch))\n",
        "\n",
        "# Close the training log file.\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/3], Step [100/6471], Loss: 8.8752, Perplexity: 7152.4333\n",
            "Epoch [1/3], Step [200/6471], Loss: 8.7506, Perplexity: 6314.4928\n",
            "Epoch [1/3], Step [300/6471], Loss: 8.7680, Perplexity: 6425.2393\n",
            "Epoch [1/3], Step [400/6471], Loss: 8.7688, Perplexity: 6430.1739\n",
            "Epoch [1/3], Step [500/6471], Loss: 8.7144, Perplexity: 6089.7664\n",
            "Epoch [1/3], Step [600/6471], Loss: 8.6830, Perplexity: 5901.7061\n",
            "Epoch [1/3], Step [700/6471], Loss: 8.6914, Perplexity: 5951.4663\n",
            "Epoch [1/3], Step [800/6471], Loss: 8.6956, Perplexity: 5976.7486\n",
            "Epoch [1/3], Step [900/6471], Loss: 8.7081, Perplexity: 6051.9834\n",
            "Epoch [1/3], Step [1000/6471], Loss: 8.7175, Perplexity: 6109.1483\n",
            "Epoch [1/3], Step [1100/6471], Loss: 8.6811, Perplexity: 5890.6680\n",
            "Epoch [1/3], Step [1200/6471], Loss: 8.6781, Perplexity: 5872.9650\n",
            "Epoch [1/3], Step [1300/6471], Loss: 8.6575, Perplexity: 5753.1134\n",
            "Epoch [1/3], Step [1400/6471], Loss: 8.7064, Perplexity: 6041.7416\n",
            "Epoch [1/3], Step [1500/6471], Loss: 8.6584, Perplexity: 5758.2676\n",
            "Epoch [1/3], Step [1600/6471], Loss: 8.7010, Perplexity: 6008.6673\n",
            "Epoch [1/3], Step [1700/6471], Loss: 8.6808, Perplexity: 5888.6853\n",
            "Epoch [1/3], Step [1800/6471], Loss: 8.6588, Perplexity: 5760.6899\n",
            "Epoch [1/3], Step [1900/6471], Loss: 8.6662, Perplexity: 5803.3473\n",
            "Epoch [1/3], Step [2000/6471], Loss: 8.6491, Perplexity: 5705.2510\n",
            "Epoch [1/3], Step [2100/6471], Loss: 8.6566, Perplexity: 5747.7774\n",
            "Epoch [1/3], Step [2200/6471], Loss: 8.6398, Perplexity: 5652.3776\n",
            "Epoch [1/3], Step [2300/6471], Loss: 8.6461, Perplexity: 5688.1540\n",
            "Epoch [1/3], Step [2400/6471], Loss: 8.6438, Perplexity: 5674.9387\n",
            "Epoch [1/3], Step [2500/6471], Loss: 8.6533, Perplexity: 5728.8154\n",
            "Epoch [1/3], Step [2600/6471], Loss: 8.6317, Perplexity: 5606.6150\n",
            "Epoch [1/3], Step [2700/6471], Loss: 8.6203, Perplexity: 5543.3097\n",
            "Epoch [1/3], Step [2800/6471], Loss: 8.6328, Perplexity: 5612.7619\n",
            "Epoch [1/3], Step [2900/6471], Loss: 8.6238, Perplexity: 5562.6285\n",
            "Epoch [1/3], Step [3000/6471], Loss: 8.6339, Perplexity: 5618.7334\n",
            "Epoch [1/3], Step [3100/6471], Loss: 8.6368, Perplexity: 5635.0252\n",
            "Epoch [1/3], Step [3200/6471], Loss: 8.6008, Perplexity: 5435.9576\n",
            "Epoch [1/3], Step [3300/6471], Loss: 8.6490, Perplexity: 5704.4295\n",
            "Epoch [1/3], Step [3400/6471], Loss: 8.6424, Perplexity: 5666.6643\n",
            "Epoch [1/3], Step [3500/6471], Loss: 8.6168, Perplexity: 5523.8947\n",
            "Epoch [1/3], Step [3600/6471], Loss: 8.6240, Perplexity: 5563.6790\n",
            "Epoch [1/3], Step [3700/6471], Loss: 8.5956, Perplexity: 5407.9269\n",
            "Epoch [1/3], Step [3800/6471], Loss: 8.5926, Perplexity: 5391.7415\n",
            "Epoch [1/3], Step [3900/6471], Loss: 8.5860, Perplexity: 5356.1945\n",
            "Epoch [1/3], Step [4000/6471], Loss: 8.6185, Perplexity: 5533.3430\n",
            "Epoch [1/3], Step [4100/6471], Loss: 8.5955, Perplexity: 5407.4061\n",
            "Epoch [1/3], Step [4200/6471], Loss: 8.6072, Perplexity: 5471.1782\n",
            "Epoch [1/3], Step [4300/6471], Loss: 8.5884, Perplexity: 5369.2206\n",
            "Epoch [1/3], Step [4400/6471], Loss: 8.5824, Perplexity: 5336.7325\n",
            "Epoch [1/3], Step [4500/6471], Loss: 8.5675, Perplexity: 5257.9841\n",
            "Epoch [1/3], Step [4600/6471], Loss: 8.6041, Perplexity: 5453.8931\n",
            "Epoch [1/3], Step [4700/6471], Loss: 8.5897, Perplexity: 5376.2043\n",
            "Epoch [1/3], Step [4800/6471], Loss: 8.5903, Perplexity: 5379.1276\n",
            "Epoch [1/3], Step [4900/6471], Loss: 8.5731, Perplexity: 5287.5566\n",
            "Epoch [1/3], Step [5000/6471], Loss: 8.5598, Perplexity: 5217.5239\n",
            "Epoch [1/3], Step [5100/6471], Loss: 8.5835, Perplexity: 5342.6956\n",
            "Epoch [1/3], Step [5200/6471], Loss: 8.5773, Perplexity: 5309.9882\n",
            "Epoch [1/3], Step [5300/6471], Loss: 8.5514, Perplexity: 5174.1518\n",
            "Epoch [1/3], Step [5400/6471], Loss: 8.5782, Perplexity: 5314.7353\n",
            "Epoch [1/3], Step [5500/6471], Loss: 8.5461, Perplexity: 5146.6858\n",
            "Epoch [1/3], Step [5600/6471], Loss: 8.5563, Perplexity: 5199.5970\n",
            "Epoch [1/3], Step [5700/6471], Loss: 8.5600, Perplexity: 5218.6734\n",
            "Epoch [1/3], Step [5800/6471], Loss: 8.5428, Perplexity: 5129.6480\n",
            "Epoch [1/3], Step [5900/6471], Loss: 8.5848, Perplexity: 5349.6347\n",
            "Epoch [1/3], Step [6000/6471], Loss: 8.5308, Perplexity: 5068.5660\n",
            "Epoch [1/3], Step [6100/6471], Loss: 8.5474, Perplexity: 5153.1589\n",
            "Epoch [1/3], Step [6200/6471], Loss: 8.5864, Perplexity: 5358.5396\n",
            "Epoch [1/3], Step [6300/6471], Loss: 8.5062, Perplexity: 4945.1550\n",
            "Epoch [1/3], Step [6400/6471], Loss: 8.5271, Perplexity: 5050.0190\n",
            "Epoch [2/3], Step [100/6471], Loss: 8.5206, Perplexity: 5016.9594\n",
            "Epoch [2/3], Step [200/6471], Loss: 8.5139, Perplexity: 4983.3415\n",
            "Epoch [2/3], Step [300/6471], Loss: 8.4915, Perplexity: 4873.1141\n",
            "Epoch [2/3], Step [400/6471], Loss: 8.4882, Perplexity: 4857.0792\n",
            "Epoch [2/3], Step [500/6471], Loss: 8.5042, Perplexity: 4935.6754\n",
            "Epoch [2/3], Step [600/6471], Loss: 8.5175, Perplexity: 5001.7533\n",
            "Epoch [2/3], Step [700/6471], Loss: 8.4828, Perplexity: 4830.7250\n",
            "Epoch [2/3], Step [800/6471], Loss: 8.4890, Perplexity: 4861.2082\n",
            "Epoch [2/3], Step [900/6471], Loss: 8.4995, Perplexity: 4912.1869\n",
            "Epoch [2/3], Step [1000/6471], Loss: 8.5034, Perplexity: 4931.6525\n",
            "Epoch [2/3], Step [1100/6471], Loss: 8.4864, Perplexity: 4848.5776\n",
            "Epoch [2/3], Step [1200/6471], Loss: 8.4923, Perplexity: 4877.2706\n",
            "Epoch [2/3], Step [1300/6471], Loss: 8.4929, Perplexity: 4880.0901\n",
            "Epoch [2/3], Step [1400/6471], Loss: 8.5046, Perplexity: 4937.5021\n",
            "Epoch [2/3], Step [1500/6471], Loss: 8.4790, Perplexity: 4812.8051\n",
            "Epoch [2/3], Step [1600/6471], Loss: 8.4930, Perplexity: 4880.5323\n",
            "Epoch [2/3], Step [1700/6471], Loss: 8.4914, Perplexity: 4872.5472\n",
            "Epoch [2/3], Step [1800/6471], Loss: 8.4855, Perplexity: 4844.0344\n",
            "Epoch [2/3], Step [1900/6471], Loss: 8.4709, Perplexity: 4774.0356\n",
            "Epoch [2/3], Step [2000/6471], Loss: 8.4514, Perplexity: 4681.6215\n",
            "Epoch [2/3], Step [2100/6471], Loss: 8.4721, Perplexity: 4779.4384\n",
            "Epoch [2/3], Step [2200/6471], Loss: 8.4809, Perplexity: 4821.8280\n",
            "Epoch [2/3], Step [2300/6471], Loss: 8.4845, Perplexity: 4839.2785\n",
            "Epoch [2/3], Step [2400/6471], Loss: 8.4644, Perplexity: 4742.7194\n",
            "Epoch [2/3], Step [2500/6471], Loss: 8.4749, Perplexity: 4792.9949\n",
            "Epoch [2/3], Step [2600/6471], Loss: 8.4792, Perplexity: 4813.6222\n",
            "Epoch [2/3], Step [2700/6471], Loss: 8.4791, Perplexity: 4813.3422\n",
            "Epoch [2/3], Step [2800/6471], Loss: 8.4448, Perplexity: 4650.9337\n",
            "Epoch [2/3], Step [2900/6471], Loss: 8.4519, Perplexity: 4683.9482\n",
            "Epoch [2/3], Step [3000/6471], Loss: 8.4733, Perplexity: 4785.5044\n",
            "Epoch [2/3], Step [3100/6471], Loss: 8.4631, Perplexity: 4736.5992\n",
            "Epoch [2/3], Step [3200/6471], Loss: 8.4650, Perplexity: 4745.6240\n",
            "Epoch [2/3], Step [3300/6471], Loss: 8.4538, Perplexity: 4692.8414\n",
            "Epoch [2/3], Step [3400/6471], Loss: 8.4463, Perplexity: 4657.6094\n",
            "Epoch [2/3], Step [3500/6471], Loss: 8.4878, Perplexity: 4854.9304\n",
            "Epoch [2/3], Step [3600/6471], Loss: 8.4446, Perplexity: 4649.6919\n",
            "Epoch [2/3], Step [3700/6471], Loss: 8.4338, Perplexity: 4599.9192\n",
            "Epoch [2/3], Step [3800/6471], Loss: 8.4340, Perplexity: 4600.9766\n",
            "Epoch [2/3], Step [3900/6471], Loss: 8.4449, Perplexity: 4651.3861\n",
            "Epoch [2/3], Step [4000/6471], Loss: 8.4240, Perplexity: 4555.2255\n",
            "Epoch [2/3], Step [4100/6471], Loss: 8.4352, Perplexity: 4606.3899\n",
            "Epoch [2/3], Step [4200/6471], Loss: 8.4161, Perplexity: 4519.2549\n",
            "Epoch [2/3], Step [4300/6471], Loss: 8.4210, Perplexity: 4541.3322\n",
            "Epoch [2/3], Step [4400/6471], Loss: 8.4222, Perplexity: 4546.7231\n",
            "Epoch [2/3], Step [4500/6471], Loss: 8.4265, Perplexity: 4566.6607\n",
            "Epoch [2/3], Step [4600/6471], Loss: 8.4192, Perplexity: 4533.3011\n",
            "Epoch [2/3], Step [4700/6471], Loss: 8.4264, Perplexity: 4565.8768\n",
            "Epoch [2/3], Step [4800/6471], Loss: 8.4318, Perplexity: 4590.9525\n",
            "Epoch [2/3], Step [4900/6471], Loss: 8.4125, Perplexity: 4503.2161\n",
            "Epoch [2/3], Step [5000/6471], Loss: 8.4114, Perplexity: 4498.1213\n",
            "Epoch [2/3], Step [5100/6471], Loss: 8.4285, Perplexity: 4575.5145\n",
            "Epoch [2/3], Step [5200/6471], Loss: 8.4280, Perplexity: 4573.2373\n",
            "Epoch [2/3], Step [5300/6471], Loss: 8.3820, Perplexity: 4367.5689\n",
            "Epoch [2/3], Step [5400/6471], Loss: 8.4118, Perplexity: 4499.7131\n",
            "Epoch [2/3], Step [5500/6471], Loss: 8.3903, Perplexity: 4404.0244\n",
            "Epoch [2/3], Step [5600/6471], Loss: 8.4522, Perplexity: 4685.5074\n",
            "Epoch [2/3], Step [5700/6471], Loss: 8.4133, Perplexity: 4506.7176\n",
            "Epoch [2/3], Step [5800/6471], Loss: 8.4018, Perplexity: 4455.2026\n",
            "Epoch [2/3], Step [5900/6471], Loss: 8.3795, Perplexity: 4356.9480\n",
            "Epoch [2/3], Step [6000/6471], Loss: 8.4031, Perplexity: 4461.0911\n",
            "Epoch [2/3], Step [6100/6471], Loss: 8.4258, Perplexity: 4563.4826\n",
            "Epoch [2/3], Step [6138/6471], Loss: 8.3847, Perplexity: 4379.5437"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "T3PJ_mN4QY55",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kTm6ArYrQY59",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(history['train_loss'])\n",
        "plt.xlabel('steps')\n",
        "plt.ylabel('train loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ff-cWc8iQY6A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<a id='step3'></a>\n",
        "## Step 3: (可选）验证你的模型\n",
        "\n",
        "为了评估潜在的过度拟合，可以选择评估验证集的性能。如果你决定来做这个**可选**任务，则需要先完成下一个notebook中的所有步骤（**3_Inference.ipynb**）。作为该notebook的一部分，你需要编写并测试使用RNN解码器生成图像标注的代码（特别是`DecoderRNN`类中的`sample`方法）。在这里，该代码会是非常有用的。\n",
        "\n",
        "如果你决定验证模型，请不要在**data_loader.py**中编辑数据加载器。相反，你需要创建一个名为**data_loader_val.py**的新文件，其中包含用于获取验证数据的数据加载器的代码。你可以访问：\n",
        "-  路径为`'/opt/cocoapi/images/train2014/'`的验证图像文件\n",
        "-  路径为`'/opt/cocoapi/annotations/captions_val2014.json'`的文件中 ，用于验证图像标注的注释文件。\n",
        "\n",
        "根据我们的建议，验证模型的方法涉及会到创建一个json文件，例如包含模型预测的验证图像的标注的[这个文件](https://github.com/cocodataset/cocoapi/blob/master/results/captions_val2014_fakecap_results.json) 。然后，你可以编写自己的脚本或使用 [在线查找](https://github.com/tylin/coco-caption) 的脚本来计算模型的BLEU分数。你可以在 [本文](https://arxiv.org/pdf/1411.4555.pdf)第4.1节中阅读有关BLEU分数以及其他评估指标（如TEOR和Cider）的更多信息。有关如何使用注释文件的更多信息，请查看COCO数据集 [网站](http://cocodataset.org/#download) 。"
      ]
    },
    {
      "metadata": {
        "id": "BJG459knQY6B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# (Optional) TODO: Validate your model."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}