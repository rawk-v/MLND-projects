{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bias-variation decomposition\n",
    "\n",
    "在周志华的机器学习书籍中，没有看明白bias-variation decomposition的推导公式，查阅资料后，使用下面的方法进行推导就明白多了。\n",
    "\n",
    "###### 用到的知识\n",
    "\n",
    "对于任意一个随机变量$X$，其对应的期望$E[X]=\\mu$，方差为$Var(X)$，那么有\n",
    "\n",
    "$Var(X)=E[X^2]-E[X]^2$\n",
    "\n",
    "###### 定义\n",
    "\n",
    "首先我们定义：\n",
    "\n",
    "$D$表示训练数据集\n",
    "\n",
    "$y=f(x)$表示确定的Infer函数，即给定采样数据$x$，通过这个函数就可以确定输出$y$；\n",
    "\n",
    "$\\hat{f}(x)$表示从数据集$D$学得的模型，即给定采样数据$x$，通过这个函数预测到其对应值为$\\hat{f}(x)$\n",
    "\n",
    "$y_D$表示数据集$D$中某个采样数据$x$对应的实际标签，注意，这个标签不一定是真实的标签，其真实的标签为$y$，两者之间的差异即为噪声$\\epsilon$，假定噪声的均值$E[\\epsilon]=0$，方差$Var(\\epsilon)=\\sigma^2$。\n",
    "\n",
    "那么对于训练数据集$D$训练所得的模型$\\hat{f}$，其期望均方误差（这个就是我们训练模型时最小化误差的计算方式）为：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E[(\\hat{f}-y_D)^2] &=E[\\hat{f}^2-2y_D\\hat{f}+y_D^2] \\\\\n",
    "&=E[\\hat{f}^2]-2E[y_D]E[\\hat{f}]+E[y_D^2]\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "由于：\n",
    "$$\n",
    "\\begin{align}\n",
    "E[\\hat{f}^2] &= Var（\\hat{f})+E[\\hat{f}]^2 \\\\\n",
    "E[y_D^2] &= Var(y_D)+E[y_D]^2 \\\\\n",
    "E[y_D] &= E[f+\\epsilon]=E[f]+E[\\epsilon]=E[f]=f\\\\\n",
    "E[y_D]E[\\hat{f}]&=E[f+\\epsilon]E[\\hat{f}]=fE[\\hat{f}]\n",
    "\\end{align}\n",
    "$$\n",
    "那么有\n",
    "$$\n",
    "\\begin{align}\n",
    "E[((\\hat{f})-y_D)^2] &=(Var（\\hat{f})+E[\\hat{f}]^2) -2fE[\\hat{f}]+(Var(y_D)+E[y_D]^2)\\\\\n",
    "\t&=(Var（\\hat{f})+E[\\hat{f}]^2) -2fE[\\hat{f}]+(Var(y_D)+f^2) \\\\\n",
    "\t&=Var(y_D)+Var(\\hat{f})+(f^2-2fE[\\hat{f}]+E[\\hat{f}]^2)\\\\\n",
    "\t&=Var(y_D)+Var(\\hat{f})+(f-\\hat{f})^2\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "上式中：\n",
    "$$\n",
    "\\begin{align}\n",
    "Bias[\\hat{f}]&=E[\\hat{f}-f]=E[\\hat{f}]-E[f]=E[\\hat{f}]-f\\\\\n",
    "Bias[\\hat{f}]^2&=(E[\\hat{f}-f])^2\\\\\n",
    "&=(f-E[\\hat{f}])^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Var(y_D)&=E[(y_D-E[y_D])^2]\\\\\n",
    "\t&= E[(y_D-y)^2] \\\\\n",
    "\t&= E[(y+\\epsilon-y)^2] \\\\\n",
    "\t&= E[\\epsilon^2] \\\\\n",
    "\t&= Var(\\epsilon)+E[\\epsilon]^2 \\\\\n",
    "\t&= \\sigma^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "所以，有\n",
    "$$\n",
    "E[(\\hat{f}-y+D)^2]=Var(\\hat{f})+Bias(\\hat{f})^2+\\sigma^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数学期望，方差，标准差\n",
    "这些概念都是针对随机变量的概念，随机变量可以使用**上帝视觉**来加深理解。通常我们计算的数学期望、方差、标准差都是基于某些数据**估计**得到的，这就是**参数估计**。而真实的数学期望、方差、标准差是使用随机变量的概率分布函数/概率密度函数计算得到的。\n",
    "\n",
    "# 到底什么是极大似然\n",
    "这是在**统计推断（Statistical Inference）**中**参数估计（Estimation）**中用到的方法。\n",
    "\n",
    "> 参考：[极大似然估计](https://blog.csdn.net/zengxiantao1994/article/details/72787849)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贝叶斯分类\n",
    "1. 极大似然估计（Maximum Likelyhood Estimation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树的优缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 集成方法 (Bagging, AdaBoost, Random Forest, Gradient Boosting)\n",
    "\n",
    "## AdaBoost（Adaptive Boosting）\n",
    "\n",
    "AdaBoost的算法步骤：\n",
    "给定一个二分类的数据集：$T={(x_1, y_1), (x_2,y_2), (x_3, y_3), \\dotso, (x_N, y_N)}$\n",
    "每个样本点由特征向量和对应的标记组合，特征向量$x_i\\in\\mathcal{X}\\subseteq\\mathbf{R}^n$，标记$y_i\\in\\mathcal{Y}={-1,+1}$。其中$\\mathcal{X}$是特征空间，$\\mathcal{Y}$是标记集合。Adaboost从训练数据中学习一系列弱分类算法/基本分类器，并最终使用权重将这些弱分类器线性组合成一个强分类器。\n",
    "输入：训练数据集$T$，对应的标签$y_i$，某种弱学习算法（例如决策树）。\n",
    "输出：最终的分类器$G(x)$。\n",
    "步骤如下：\n",
    "1. 初始化训练数据集中每个采样点的误差权重（该权重用于产生改变该采样点在后续处理过程中的重要性）：\n",
    "$$\n",
    "D_1=(w_{1,1},w_{1,2},w_{1,3},\\dotso,w_{1,N}), w_{1,i}=\\frac{1}{N},i=1,2,3,\\dotso,N\n",
    "$$\n",
    " $D_1$的下标$1$表示训练的轮数，$D_1$即第一轮\n",
    "2. 设训练轮数$m=1,2,3,\\dotso,M$\n",
    " 1. 使用权重分布$D_m$对训练数据集$T$进行训练，得到弱分类器：\n",
    "   $$\n",
    "   G_m(x):\\mathcal{X}\\mapsto{-1,+1}\n",
    "   $$\n",
    " 2. 计算$G_m(x)$在训练集上的分类误差\n",
    "   $$\n",
    "   e_m=\\sum_{i=1}^{N}P(G_m(x)\\neq y_i)=\\sum_{i=1}^{N}w_{m,i}I(G_m(x_i)\\neq y_i)\n",
    "   $$\n",
    "   上式中的$I(G_m(x_i)\\neq y_i)$是指示函数，表示当$G_m(x_i)=y_i$时，$I(x)=1$，否则$I(x)=0$，那么$\\sum_{i=1}^{N}w_{m,i}I(G_m(x_i)\\neq y_i)$就等于弱分类器在所有错误分类的样本中，对应的误差权重$w_{m,i}$之和。\n",
    " 3. 计算弱分类器$G_m(x)$在最终输出的强分类器中的权重系数，表示这一弱分类器在最终强分类器中的重要程度：\n",
    "   $$\n",
    "   \\alpha=\\frac{1}{2}ln\\frac{1-e_m}{e_m}\n",
    "   $$\n",
    " 4. 更新计算出下一轮的弱分类器所需的样本权重：\n",
    "   $$\n",
    "   D_{m+1}=(w_{m+1,1},w_{m+1,2},w_{m+1,3},\\dotso,w_{m+1,N}) \\\\\n",
    "   w_{m+1,i}=\\frac{w_{m,i}}{Z_m}e^{-\\alpha_{m}y_{i}G_{m}(x_i)}, i=1,2,3,\\dotso,N\n",
    "   $$\n",
    "   其中，$Z_m$是上一轮权重的归一化因子\n",
    "   $$\n",
    "   Z_m=\\sum_{i=1}^{N}w_{m,i}e^{-\\alpha_{m}y_{i}G_{m}(x)}\n",
    "   $$\n",
    "3. 经过$M$轮训练之后，就可以得到$M$个弱分类器，和对应的$M$个权重系数$\\alpha_i$，最终的强分类器为：\n",
    "  $$\n",
    "  f(x)=\\sum_{m=1}^{M}\\alpha_{m}G_{m}(x)\n",
    "  $$\n",
    "  \n",
    "由上面计算每个弱分类器的权重系数$\\alpha_m=\\frac{1}{2}ln\\frac{1-e_m}{e_m}$，可以知道，由于弱分类器必须满足$e_m\\leq \\frac{1}{2}$，所以有$\\alpha_m\\geq 0$，并且随着$e_m$不断减小，$\\alpha_m$将不断增大，也就是说，每当当前训练得到的弱分流器对于样本集的分类误差率更小时，它在最终获得的强分类器中的重要性就越高。\n",
    "\n",
    "下面举例说明这一过程。\n",
    "给定下表所示的数据集：\n",
    "\n",
    "|序号|1|2|3|4|5|6|7|8|9|10|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|$x$|0|1|2|3|4|5|6|7|8|9|\n",
    "|$y$|1|1|1|-1|-1|-1|1|1|1|-1|\n",
    "\n",
    "假定弱分类器由$x<v$或$x>v$生成，阈值$v$在使得当前弱分类器在训练数据集上的分类误差最小时取得。（**弱分类器的生成规则需要进一步研究**）\n",
    "使用AdaBoost的实现过程：\n",
    "1. 第一轮，$m=1$\n",
    " 1. 初始化样本的权重值\n",
    "   $$\n",
    "   D_1=(w_{1,1},w_{1,2},w_{1,3},\\dotso,w_{1,10}), w_{1,i}=\\frac{1}{10}=0.1\n",
    "   $$\n",
    " 2. 在权重$D_1$的基础上，训练弱分类器，训练结果如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result positive:-1, threshold:2.5, min error:0.30000000000000004\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "x = np.arange(10)\n",
    "y = np.array([1,1,1,-1,-1,-1,1,1,1,-1])\n",
    "\n",
    "def predict_according_to_v(x, v, positive=1):\n",
    "    result = np.zeros(x.shape[0])\n",
    "    for i, val in enumerate(x):\n",
    "        if val > v:\n",
    "            result[i] = positive\n",
    "        else:\n",
    "            result[i] = -positive\n",
    "    return result\n",
    "min_error = 1\n",
    "threshold = 0\n",
    "positive = 1\n",
    "for v in range(12):\n",
    "    v = v - 0.5\n",
    "    pred = predict_according_to_v(x, v)\n",
    "    error = 1-accuracy_score(y, pred)\n",
    "    if error < min_error:\n",
    "        min_error = error\n",
    "        threshold = v\n",
    "        positive = 1\n",
    "    pred = predict_according_to_v(x, v, positive=-1)\n",
    "    error = 1-accuracy_score(y, pred)\n",
    "    if error < min_error:\n",
    "        min_error = error\n",
    "        threshold = v\n",
    "        positive = -1\n",
    "print(\"result positive:{}, threshold:{}, min error:{}\".format(positive, threshold, min_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. 续前文。\n",
    "  2. 由以上结果可以知道,当阈值$v=0.25$时分类误差最小，因此得到弱分类器\n",
    "  $$\n",
    "  G_1(x) = \\left\\{ \\begin{array}{rcl}\n",
    "  1, &  x<2.5 \\\\\n",
    "  -1, &  x >2.5\n",
    "  \\end{array}\\right.\n",
    "  $$\n",
    "  \n",
    "  3. 有以上结果可以知道，$G_1(x)$在训练集上的训练误差率为$e_1=P(G_i(x_i))\\neq y_i)=0.3$\n",
    "  4. 计算$G_1(x)$的权重系数：$\\alpha_1=\\frac{1}{2}ln\\frac{1-e_1}{e_1}=0.4236$\n",
    "  5. 更新样本点的误差权重，用以下一轮训练使用：\n",
    "    $$\n",
    "    D_2=(w_{2,1},w_{2,2},w_{2,3},\\dotso,w_{2,N}) \\\\\n",
    "    w_{2,i}=\\frac{w_{1,i}}{Z_1}e^{-\\alpha_{1}y_{i}G_{1}(x_i)}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9165151389911682\n",
      "[0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857\n",
      " 0.16666667 0.16666667 0.16666667 0.07142857]\n"
     ]
    }
   ],
   "source": [
    "D1=np.ones(10)/10\n",
    "# D1 对应的预测\n",
    "G1 = predict_according_to_v(x, 2.5, -1)\n",
    "e1 = 1- accuracy_score(y, G1)\n",
    "a1 = 1.0/2*np.log((1-e1)/e1)\n",
    "Z1 = np.sum(D1 * np.exp(-a1*y*G1))\n",
    "D2 = D1/Z1*np.exp(-a1*y*G1)\n",
    "print(Z1)\n",
    "print(D2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "于是得到第一轮结束后的分类器：$sign[f_1(x)]$，该分类器在训练集上仍然由3个误分类点，因此需要继续下一轮。\n",
    "2. 第二轮，$m=2$\n",
    " 1. 在误差权重$D_2$上训练弱分类器，当分类误差最小时，可以得到如下弱分类器：\n",
    "     $$\n",
    "  G_2(x) = \\left\\{ \\begin{array}{rcl}\n",
    "  1, &  x<8.5 \\\\\n",
    "  -1, &  x >8.5\n",
    "  \\end{array}\\right.\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e2:0.21428571\n",
      "a2:0.6496415047924033\n",
      "D3:[0.04545454 0.04545454 0.04545454 0.16666667 0.16666667 0.16666667\n",
      " 0.10606061 0.10606061 0.10606061 0.04545454]\n"
     ]
    }
   ],
   "source": [
    "D2 = np.array([0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,\n",
    "    0.16666667, 0.16666667, 0.16666667, 0.07142857])\n",
    "G2 = predict_according_to_v(x, 8.5, -1)\n",
    "e2 = 0\n",
    "for i, val in enumerate(y):\n",
    "    if val != G2[i]:\n",
    "        e2 += D2[i]\n",
    "a2 = 1.0/2*np.log((1-e2)/e2)\n",
    "Z2 = np.sum(D2 * np.exp(-a2*y*G2))\n",
    "D3 = D2/Z2*np.exp(-a2*y*G2)\n",
    "\n",
    "print(\"e2:{}\\na2:{}\\nD3:{}\".format(e2, a2, D3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 续前文\n",
    " 2. 计算得到$G_2(x)$在训练数据集上的误差率为$e_2=0.2143$.\n",
    " 3. 计算弱分类器的权重系数得到：$\\alpha_2=0.6496$.\n",
    " 4. 计算得到第三轮的误差权重：\n",
    "  $$D_3=(0.04545454, 0.04545454, 0.04545454, 0.16666667, 0.16666667, 0.16666667\n",
    ", 0.10606061, 0.10606061, 0.10606061, 0.04545454) \\\\\n",
    "  f_2(x)=\\alpha_{1}G_1(x)+\\alpha_{2}G_2(x)=0.4236G_1(x)+0.6496G_2(x)\n",
    "  $$\n",
    "  得到的分类器$sign[f_2(x)]$在训练数据集上仍然由3个误分类点，所以要继续下一轮计算\n",
    "\n",
    "3. 第三轮，$m=3$\n",
    " 1. 在误差权重$D_3$下训练数据，在误差率最低时，得到如下弱分类器：\n",
    "  $$\n",
    "  G_3(x) = \\left\\{ \\begin{array}{rcl}\n",
    "  1, &  x>5.5 \\\\\n",
    "  -1, &  x<5.5\n",
    "  \\end{array}\\right.\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e3:0.18181816\n",
      "a3:0.7520387717214738\n",
      "D4:[0.125      0.125      0.125      0.10185185 0.10185185 0.10185185\n",
      " 0.06481482 0.06481482 0.06481482 0.125     ]\n"
     ]
    }
   ],
   "source": [
    "D3 = np.array([0.04545454,0.04545454,0.04545454,0.16666667,0.16666667,0.16666667,\n",
    "               0.10606061,0.10606061,0.10606061,0.04545454])\n",
    "G3 = predict_according_to_v(x, 5.5, 1)\n",
    "e3 = 0\n",
    "for i, val in enumerate(y):\n",
    "    if val != G3[i]:\n",
    "        e3 += D3[i]\n",
    "a3 = 1.0/2*np.log((1-e3)/e3)\n",
    "Z3 = np.sum(D3 * np.exp(-a3*y*G3))\n",
    "D4 = D3/Z3*np.exp(-a3*y*G3)\n",
    "\n",
    "print(\"e3:{}\\na3:{}\\nD4:{}\".format(e3, a3, D4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 续前文\n",
    " 2. 计算得到$G_3(x)$在训练数据集上的误差率为$e_3=0.1820$.\n",
    " 3. 计算弱分类器的权重系数得到：$\\alpha_3=0.7520$.\n",
    " 4. 计算得到第三轮的误差权重：\n",
    "  $$D_4=(0.125, 0.125, 0.125, 0.10185185, 0.10185185, 0.10185185\n",
    ", 0.06481482,0.06481482, 0.06481482, 0.125) \\\\\n",
    "  f_3(x)=\\alpha_{1}G_1(x)+\\alpha_{2}G_2(x)+\\alpha_{3}G_3(x)=0.4236G_1(x)+0.6496G_2(x)+0.7520G_3(x)\n",
    "  $$\n",
    "  得到的分类器$sign[f_3(x)]$在训练数据集上已经没有误分类点了，所以不需要进行下一轮计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K近邻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机梯度下降分类器 (SGDC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 支持向量机（SVM）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
