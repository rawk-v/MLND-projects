{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bias-variation decomposition\n",
    "\n",
    "在周志华的机器学习书籍中，没有看明白bias-variation decomposition的推导公式，查阅资料后，使用下面的方法进行推导就明白多了。\n",
    "\n",
    "###### 用到的知识\n",
    "\n",
    "对于任意一个随机变量$X$，其对应的期望$E[X]=\\mu$，方差为$Var(X)$，那么有\n",
    "\n",
    "$Var(X)=E[X^2]-E[X]^2$\n",
    "\n",
    "###### 定义\n",
    "\n",
    "首先我们定义：\n",
    "\n",
    "$D$表示训练数据集\n",
    "\n",
    "$y=f(x)$表示确定的Infer函数，即给定采样数据$x$，通过这个函数就可以确定输出$y$；\n",
    "\n",
    "$\\hat{f}(x)$表示从数据集$D$学得的模型，即给定采样数据$x$，通过这个函数预测到其对应值为$\\hat{f}(x)$\n",
    "\n",
    "$y_D$表示数据集$D$中某个采样数据$x$对应的实际标签，注意，这个标签不一定是真实的标签，其真实的标签为$y$，两者之间的差异即为噪声$\\epsilon$，假定噪声的均值$E[\\epsilon]=0$，方差$Var(\\epsilon)=\\sigma^2$。\n",
    "\n",
    "那么对于训练数据集$D$训练所得的模型$\\hat{f}$，其期望均方误差（这个就是我们训练模型时最小化误差的计算方式）为：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E[(\\hat{f}-y_D)^2] &=E[\\hat{f}^2-2y_D\\hat{f}+y_D^2] \\\\\n",
    "&=E[\\hat{f}^2]-2E[y_D]E[\\hat{f}]+E[y_D^2]\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "由于：\n",
    "$$\n",
    "\\begin{align}\n",
    "E[\\hat{f}^2] &= Var（\\hat{f})+E[\\hat{f}]^2 \\\\\n",
    "E[y_D^2] &= Var(y_D)+E[y_D]^2 \\\\\n",
    "E[y_D] &= E[f+\\epsilon]=E[f]+E[\\epsilon]=E[f]=f\\\\\n",
    "E[y_D]E[\\hat{f}]&=E[f+\\epsilon]E[\\hat{f}]=fE[\\hat{f}]\n",
    "\\end{align}\n",
    "$$\n",
    "那么有\n",
    "$$\n",
    "\\begin{align}\n",
    "E[((\\hat{f})-y_D)^2] &=(Var（\\hat{f})+E[\\hat{f}]^2) -2fE[\\hat{f}]+(Var(y_D)+E[y_D]^2)\\\\\n",
    "\t&=(Var（\\hat{f})+E[\\hat{f}]^2) -2fE[\\hat{f}]+(Var(y_D)+f^2) \\\\\n",
    "\t&=Var(y_D)+Var(\\hat{f})+(f^2-2fE[\\hat{f}]+E[\\hat{f}]^2)\\\\\n",
    "\t&=Var(y_D)+Var(\\hat{f})+(f-\\hat{f})^2\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "上式中：\n",
    "$$\n",
    "\\begin{align}\n",
    "Bias[\\hat{f}]&=E[\\hat{f}-f]=E[\\hat{f}]-E[f]=E[\\hat{f}]-f\\\\\n",
    "Bias[\\hat{f}]^2&=(E[\\hat{f}-f])^2\\\\\n",
    "&=(f-E[\\hat{f}])^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Var(y_D)&=E[(y_D-E[y_D])^2]\\\\\n",
    "\t&= E[(y_D-y)^2] \\\\\n",
    "\t&= E[(y+\\epsilon-y)^2] \\\\\n",
    "\t&= E[\\epsilon^2] \\\\\n",
    "\t&= Var(\\epsilon)+E[\\epsilon]^2 \\\\\n",
    "\t&= \\sigma^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "所以，有\n",
    "$$\n",
    "E[(\\hat{f}-y+D)^2]=Var(\\hat{f})+Bias(\\hat{f})^2+\\sigma^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数学期望，方差，标准差\n",
    "这些概念都是针对随机变量的概念，随机变量可以使用**上帝视觉**来加深理解。通常我们计算的数学期望、方差、标准差都是基于某些数据**估计**得到的，这就是**参数估计**。而真实的数学期望、方差、标准差是使用随机变量的概率分布函数/概率密度函数计算得到的。\n",
    "\n",
    "# 到底什么是极大似然\n",
    "这是在**统计推断（Statistical Inference）**中**参数估计（Estimation）**中用到的方法。\n",
    "\n",
    "> 参考：[极大似然估计](https://blog.csdn.net/zengxiantao1994/article/details/72787849)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正则化\n",
    "\n",
    "对于多项式：$f(x)=2x+3x^2-4x^3, w=(2, 3, -4)$\n",
    "1. $L_1$正则化，$L_1= \\|w\\|_1 = |2|+|3|+|-4| = 9$\n",
    "2. $L_2$正则化，$L_2= \\|w\\|_2 = \\sqrt{(2)^2+(3)^2+(-4)^2}=\\sqrt{29}$。\n",
    "$L_1$和$L_2$正则化用于对多项式的复杂度生成一个度量，可以作为生成泛化能力强的模型的一个罚项（用于抑制模型的复杂度），例如，把预测误差与正则化数相加，得到新的误差标准，当模型复杂时，对于训练数据拟合程度高，但是正则化系数会很大，误差项就会很大，迫使学习算法将模型往更简单的方向倾斜。\n",
    "\n",
    "\n",
    "> 参考：https://www.kaggle.com/residentmario/l1-norms-versus-l2-norms\n",
    "\n",
    "1. $L_1$正则化\n",
    "\n",
    "2. $L_2$正则化可以使所有的权重系数都同样变得很小，如下图所示：\n",
    "\n",
    " <img src=\"./images/normalizaition0.png\" width=\"50%\" />\n",
    " \n",
    " 如上图所示，加入有权重系数$(1, 0), L_2=(1^2+0^2)=1$，而对于另外一组权重系数$(0.5, 0.5), L_2=(0.5^2+0.5^2)=0.5$，可以看到，后面一组权重系数更小，即改组权重系数可以得到更小的误差（复杂度罚项误差），那么学习算法就会倾向于选择后面一组权重系数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贝叶斯分类\n",
    "1. 极大似然估计（Maximum Likelyhood Estimation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树的优缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 集成方法 (Bagging, AdaBoost, Random Forest, Gradient Boosting)\n",
    "\n",
    "## AdaBoost（Adaptive Boosting）\n",
    "\n",
    "AdaBoost的算法步骤：\n",
    "给定一个二分类的数据集：$T={(x_1, y_1), (x_2,y_2), (x_3, y_3), \\dotso, (x_N, y_N)}$\n",
    "每个样本点由特征向量和对应的标记组合，特征向量$x_i\\in\\mathcal{X}\\subseteq\\mathbf{R}^n$，标记$y_i\\in\\mathcal{Y}={-1,+1}$。其中$\\mathcal{X}$是特征空间，$\\mathcal{Y}$是标记集合。Adaboost从训练数据中学习一系列弱分类算法/基本分类器，并最终使用权重将这些弱分类器线性组合成一个强分类器。\n",
    "输入：训练数据集$T$，对应的标签$y_i$，某种弱学习算法（例如决策树）。\n",
    "输出：最终的分类器$G(x)$。\n",
    "步骤如下：\n",
    "1. 初始化训练数据集中每个采样点的误差权重（该权重用于产生改变该采样点在后续处理过程中的重要性）：\n",
    "$$\n",
    "D_1=(w_{1,1},w_{1,2},w_{1,3},\\dotso,w_{1,N}), w_{1,i}=\\frac{1}{N},i=1,2,3,\\dotso,N\n",
    "$$\n",
    " $D_1$的下标$1$表示训练的轮数，$D_1$即第一轮\n",
    "2. 设训练轮数$m=1,2,3,\\dotso,M$\n",
    " 1. 使用权重分布$D_m$对训练数据集$T$进行训练，得到弱分类器：\n",
    "   $$\n",
    "   G_m(x):\\mathcal{X}\\mapsto{-1,+1}\n",
    "   $$\n",
    " 2. 计算$G_m(x)$在训练集上的分类误差\n",
    "   $$\n",
    "   e_m=\\sum_{i=1}^{N}P(G_m(x)\\neq y_i)=\\sum_{i=1}^{N}w_{m,i}I(G_m(x_i)\\neq y_i)\n",
    "   $$\n",
    "   上式中的$I(G_m(x_i)\\neq y_i)$是指示函数，表示当$G_m(x_i)=y_i$时，$I(x)=1$，否则$I(x)=0$，那么$\\sum_{i=1}^{N}w_{m,i}I(G_m(x_i)\\neq y_i)$就等于弱分类器在所有错误分类的样本中，对应的误差权重$w_{m,i}$之和。\n",
    " 3. 计算弱分类器$G_m(x)$在最终输出的强分类器中的权重系数，表示这一弱分类器在最终强分类器中的重要程度：\n",
    "   $$\n",
    "   \\alpha=\\frac{1}{2}ln\\frac{1-e_m}{e_m}\n",
    "   $$\n",
    " 4. 更新计算出下一轮的弱分类器所需的样本权重：\n",
    "   $$\n",
    "   D_{m+1}=(w_{m+1,1},w_{m+1,2},w_{m+1,3},\\dotso,w_{m+1,N}) \\\\\n",
    "   w_{m+1,i}=\\frac{w_{m,i}}{Z_m}e^{-\\alpha_{m}y_{i}G_{m}(x_i)}, i=1,2,3,\\dotso,N\n",
    "   $$\n",
    "   其中，$Z_m$是上一轮权重的归一化因子\n",
    "   $$\n",
    "   Z_m=\\sum_{i=1}^{N}w_{m,i}e^{-\\alpha_{m}y_{i}G_{m}(x)}\n",
    "   $$\n",
    "3. 经过$M$轮训练之后，就可以得到$M$个弱分类器，和对应的$M$个权重系数$\\alpha_i$，最终的强分类器为：\n",
    "  $$\n",
    "  f(x)=\\sum_{m=1}^{M}\\alpha_{m}G_{m}(x)\n",
    "  $$\n",
    "  \n",
    "由上面计算每个弱分类器的权重系数$\\alpha_m=\\frac{1}{2}ln\\frac{1-e_m}{e_m}$，可以知道，由于弱分类器必须满足$e_m\\leq \\frac{1}{2}$，所以有$\\alpha_m\\geq 0$，并且随着$e_m$不断减小，$\\alpha_m$将不断增大，也就是说，每当当前训练得到的弱分流器对于样本集的分类误差率更小时，它在最终获得的强分类器中的重要性就越高。\n",
    "\n",
    "下面举例说明这一过程。\n",
    "给定下表所示的数据集：\n",
    "\n",
    "|序号|1|2|3|4|5|6|7|8|9|10|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|$x$|0|1|2|3|4|5|6|7|8|9|\n",
    "|$y$|1|1|1|-1|-1|-1|1|1|1|-1|\n",
    "\n",
    "假定弱分类器由$x<v$或$x>v$生成，阈值$v$在使得当前弱分类器在训练数据集上的分类误差最小时取得。（**弱分类器的生成规则需要进一步研究**）\n",
    "使用AdaBoost的实现过程：\n",
    "1. 第一轮，$m=1$\n",
    " 1. 初始化样本的权重值\n",
    "   $$\n",
    "   D_1=(w_{1,1},w_{1,2},w_{1,3},\\dotso,w_{1,10}), w_{1,i}=\\frac{1}{10}=0.1\n",
    "   $$\n",
    " 2. 在权重$D_1$的基础上，训练弱分类器，训练结果如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result positive:-1, threshold:2.5, min error:0.30000000000000004\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "x = np.arange(10)\n",
    "y = np.array([1,1,1,-1,-1,-1,1,1,1,-1])\n",
    "\n",
    "def predict_according_to_v(x, v, positive=1):\n",
    "    result = np.zeros(x.shape[0])\n",
    "    for i, val in enumerate(x):\n",
    "        if val > v:\n",
    "            result[i] = positive\n",
    "        else:\n",
    "            result[i] = -positive\n",
    "    return result\n",
    "min_error = 1\n",
    "threshold = 0\n",
    "positive = 1\n",
    "for v in range(12):\n",
    "    v = v - 0.5\n",
    "    pred = predict_according_to_v(x, v)\n",
    "    error = 1-accuracy_score(y, pred)\n",
    "    if error < min_error:\n",
    "        min_error = error\n",
    "        threshold = v\n",
    "        positive = 1\n",
    "    pred = predict_according_to_v(x, v, positive=-1)\n",
    "    error = 1-accuracy_score(y, pred)\n",
    "    if error < min_error:\n",
    "        min_error = error\n",
    "        threshold = v\n",
    "        positive = -1\n",
    "print(\"result positive:{}, threshold:{}, min error:{}\".format(positive, threshold, min_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  2. 由以上结果可以知道,当阈值$v=0.25$时分类误差最小，因此得到弱分类器\n",
    "  $$\n",
    "  G_1(x) = \\left\\{ \\begin{array}{rcl}\n",
    "  1, &  x<2.5 \\\\\n",
    "  -1, &  x >2.5\n",
    "  \\end{array}\\right.\n",
    "  $$\n",
    "  \n",
    "  3. 有以上结果可以知道，$G_1(x)$在训练集上的训练误差率为$e_1=P(G_i(x_i))\\neq y_i)=0.3$\n",
    "  4. 计算$G_1(x)$的权重系数：$\\alpha_1=\\frac{1}{2}ln\\frac{1-e_1}{e_1}=0.4236$\n",
    "  5. 更新样本点的误差权重，用以下一轮训练使用：\n",
    "    $$\n",
    "    D_2=(w_{2,1},w_{2,2},w_{2,3},\\dotso,w_{2,N}) \\\\\n",
    "    w_{2,i}=\\frac{w_{1,i}}{Z_1}e^{-\\alpha_{1}y_{i}G_{1}(x_i)}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9165151389911682\n",
      "[0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857\n",
      " 0.16666667 0.16666667 0.16666667 0.07142857]\n"
     ]
    }
   ],
   "source": [
    "D1=np.ones(10)/10\n",
    "# D1 对应的预测\n",
    "G1 = predict_according_to_v(x, 2.5, -1)\n",
    "e1 = 1- accuracy_score(y, G1)\n",
    "a1 = 1.0/2*np.log((1-e1)/e1)\n",
    "Z1 = np.sum(D1 * np.exp(-a1*y*G1))\n",
    "D2 = D1/Z1*np.exp(-a1*y*G1)\n",
    "print(Z1)\n",
    "print(D2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "于是得到第一轮结束后的分类器：$sign[f_1(x)]$，该分类器在训练集上仍然由3个误分类点，因此需要继续下一轮。\n",
    "2. 第二轮，$m=2$\n",
    " 1. 在误差权重$D_2$上训练弱分类器，当分类误差最小时，可以得到如下弱分类器：\n",
    "     $$\n",
    "  G_2(x) = \\left\\{ \\begin{array}{rcl}\n",
    "  1, &  x<8.5 \\\\\n",
    "  -1, &  x >8.5\n",
    "  \\end{array}\\right.\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e2:0.21428571\n",
      "a2:0.6496415047924033\n",
      "D3:[0.04545454 0.04545454 0.04545454 0.16666667 0.16666667 0.16666667\n",
      " 0.10606061 0.10606061 0.10606061 0.04545454]\n"
     ]
    }
   ],
   "source": [
    "D2 = np.array([0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857,\n",
    "    0.16666667, 0.16666667, 0.16666667, 0.07142857])\n",
    "G2 = predict_according_to_v(x, 8.5, -1)\n",
    "e2 = 0\n",
    "for i, val in enumerate(y):\n",
    "    if val != G2[i]:\n",
    "        e2 += D2[i]\n",
    "a2 = 1.0/2*np.log((1-e2)/e2)\n",
    "Z2 = np.sum(D2 * np.exp(-a2*y*G2))\n",
    "D3 = D2/Z2*np.exp(-a2*y*G2)\n",
    "\n",
    "print(\"e2:{}\\na2:{}\\nD3:{}\".format(e2, a2, D3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. 2. 计算得到$G_2(x)$在训练数据集上的误差率为$e_2=0.2143$.\n",
    " 3. 计算弱分类器的权重系数得到：$\\alpha_2=0.6496$.\n",
    " 4. 计算得到第三轮的误差权重：\n",
    "  $$D_3=(0.04545454, 0.04545454, 0.04545454, 0.16666667, 0.16666667, 0.16666667\n",
    ", 0.10606061, 0.10606061, 0.10606061, 0.04545454) \\\\\n",
    "  f_2(x)=\\alpha_{1}G_1(x)+\\alpha_{2}G_2(x)=0.4236G_1(x)+0.6496G_2(x)\n",
    "  $$\n",
    "  得到的分类器$sign[f_2(x)]$在训练数据集上仍然由3个误分类点，所以要继续下一轮计算\n",
    "\n",
    "3. 第三轮，$m=3$\n",
    " 1. 在误差权重$D_3$下训练数据，在误差率最低时，得到如下弱分类器：\n",
    "  $$\n",
    "  G_3(x) = \\left\\{ \\begin{array}{rcl}\n",
    "  1, &  x>5.5 \\\\\n",
    "  -1, &  x<5.5\n",
    "  \\end{array}\\right.\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e3:0.18181816\n",
      "a3:0.7520387717214738\n",
      "D4:[0.125      0.125      0.125      0.10185185 0.10185185 0.10185185\n",
      " 0.06481482 0.06481482 0.06481482 0.125     ]\n"
     ]
    }
   ],
   "source": [
    "D3 = np.array([0.04545454,0.04545454,0.04545454,0.16666667,0.16666667,0.16666667,\n",
    "               0.10606061,0.10606061,0.10606061,0.04545454])\n",
    "G3 = predict_according_to_v(x, 5.5, 1)\n",
    "e3 = 0\n",
    "for i, val in enumerate(y):\n",
    "    if val != G3[i]:\n",
    "        e3 += D3[i]\n",
    "a3 = 1.0/2*np.log((1-e3)/e3)\n",
    "Z3 = np.sum(D3 * np.exp(-a3*y*G3))\n",
    "D4 = D3/Z3*np.exp(-a3*y*G3)\n",
    "\n",
    "print(\"e3:{}\\na3:{}\\nD4:{}\".format(e3, a3, D4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 2. 计算得到$G_3(x)$在训练数据集上的误差率为$e_3=0.1820$.\n",
    " 3. 计算弱分类器的权重系数得到：$\\alpha_3=0.7520$.\n",
    " 4. 计算得到第三轮的误差权重：\n",
    "  $$D_4=(0.125, 0.125, 0.125, 0.10185185, 0.10185185, 0.10185185\n",
    ", 0.06481482,0.06481482, 0.06481482, 0.125) \\\\\n",
    "  f_3(x)=\\alpha_{1}G_1(x)+\\alpha_{2}G_2(x)+\\alpha_{3}G_3(x)=0.4236G_1(x)+0.6496G_2(x)+0.7520G_3(x)\n",
    "  $$\n",
    "  得到的分类器$sign[f_3(x)]$在训练数据集上已经没有误分类点了，所以不需要进行下一轮计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K近邻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机梯度下降分类器 (SGDC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 支持向量机（SVM）\n",
    "\n",
    "### 凸二次规划问题（Convex quadratic programming）\n",
    "\n",
    "### 希尔伯特空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic回归（Logistic Regression）\n",
    "\n",
    "Logistic Regression不是回归模型，是用于分类的模型。主要步骤：\n",
    "1. 假设训练数据包含$n$个特征$X=(x_1,x_2,\\dots,x_n)$，使用这$n$个特征与权重$W=(w_1,w_2,\\dots,w_n), b$构成一个线性输出$\\phi(X)=\\mathbf{W}\\mathbf{X}+b$；\n",
    "2. 把这个线性输出作为分类的依据：\n",
    " $$\n",
    "y=\\left\\{ \\begin{array}{rcl}\n",
    "1, & \\phi(X^{(i)})>0\\\\\n",
    "0, & \\phi(X^{(i)}<=0\\\\\n",
    " \\end{array}\\right.\n",
    " $$\n",
    " 其中，$X^{(i)}$表示某个数据点\n",
    "3. 使用$logistic(x)$函数将输出$\\phi(X)$映射成某个概率值，使得，当这个概率值越大时，表示当前数据点$X$属于$y=1$类别的可能性，并且：\n",
    " $$\n",
    "y=\\left\\{ \\begin{array}{rcl}\n",
    "1, & logistic(\\phi(X^{(i)}))>0.5\\\\\n",
    "0, & logistic(\\phi(X^{(i)})<=0.5\\\\\n",
    " \\end{array}\\right.\n",
    " $$\n",
    "4. 找到能够使得\n",
    " $$\\prod_{i=1}^{M}{logistic(X^{(i)})^{y^{(i)}}*(1-logistic(X^{(i)}))^{1-y^{(i)}}}$$\n",
    " 达到最大值的$(\\mathbf{W},b)$，其中$M$表示训练数据的数量。$logistic(X^{(i)})^{y^{(i)}}*(1-logistic(X^{(i)}))^{1-y^{(i)}}$这种写法意思是：当$X^{(i)}$被判断为类别$y=1$时，要乘的项为$X^{(i)}$被判断为$y=1$类的概率；\n",
    "当$X^{(i)}$被判断为$y=0$时，要乘的项就是$X^{(i)}$被判断为$y=0$的概率。这个值其实就是Likelyhood，要做的就是要**Maximize Likelyhood**。\n",
    "这里因为$y^{(i)}$取值为$0$或者$1$，所以当$X^{(i)}$被判别为$y=1$类别时，第二项就为$1$了，而当$X^{(i)}$被判断为$y=0$类别时，第一项就为$1$了。\n",
    "5. 假设有训练数据和对应标签：$(X^{(1)}, 0), (X^{(2)}, 1), (X^{(3)}, 1), (X^{(4)}, 0), (X^{(5)}, 1)$，那么按照步骤4，就是要使得概率乘积：$(1-logistic(X^{(1)}))*logistic(X^{(2)})*logistic(X^{(3)})*(1-logistic(X^{(4)}))*logistic(X^{(5)})$达到最大值时的$\\mathbf{W},b)$\n",
    "6. 步骤4中的概率值是乘积的形式，不好计算，可以取对数，得到**Log-Likelyhood**：\n",
    " $$\n",
    " \\sum_{i=1}^{M}y^{(i)}*logistic(X^{(i)})+(1-y^{(i)})*(1-logistic(X^{(i)}))\n",
    " $$\n",
    "7. 步骤6中的式子可以改写成：\n",
    "  $$\n",
    " \\sum_{i=1}^{M}-y^{(i)}*logistic(X^{(i)})-(1-y^{(i)})*(1-logistic(X^{(i)}))\n",
    " $$\n",
    " 这样，问题就变成了：**Minimize log-likelyhood**。这个函数其实就是Logistic回归算法的**损失函数**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的logistic函数，就是：\n",
    "$$\n",
    "logistic(x)=\\frac{1}{1+e^-x}\n",
    "$$\n",
    "为什么可以把步骤3中的$\\phi(X^{(i)})$映射成一个概率呢？\n",
    "假设有如下的数据\n",
    "\n",
    "|A|B|Label|\n",
    "|:-:|:-:|:-:|\n",
    "|1|1.5|1|\n",
    "|1.5|1|0|\n",
    "|3|3.8|1|\n",
    "|5.5|6|1|\n",
    "|7|6|0|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "data = [[1, 1.5, 1], [1.5, 1., 0], [3, 3.8, 1], [5.5, 6, 1], [7, 6, 0]]\n",
    "df = pd.DataFrame(data, columns=['A', 'B', 'Label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/distance-line.png\" width=\"50%\" />\n",
    "\n",
    "如上图左侧所示，当使用$\\mathbf{W}\\mathbf{X}+b=0$对数据进行划分时，那么有：如果某个数据点$X^{(1)}$落在$\\mathbf{WX}+b=1$，即$\\mathbf{WX^{(1)}}+b=1$，数据点$X^{(2)}$落在$\\mathbf{WX}+b=2$，即$\\mathbf{WX^{(2)}}+b=2$时，则有：$X^{(2)}$相比$X^{(1)}$距离$\\mathbf{W}\\mathbf{X}+b=0$要更远，我们想要$X^{(2)}$属于类别$y=1$的概率也要比$X^{(1)}$更大。\n",
    "而刚好$logistic(x)$函数可以实现这样的映射：\n",
    "\n",
    "<img src=\"./images/sigmoid-func.png\" width=\"50%\" />\n",
    "\n",
    "参考：\n",
    "> https://rasbt.github.io/mlxtend/user_guide/classifier/LogisticRegression/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SoftMax回归\n",
    "\n",
    "SoftMax是多分类算法，Logistic回归解决的是二分类问题，它其实是SoftMax回归的特殊情况。\n",
    "参考：\n",
    "> https://rasbt.github.io/mlxtend/user_guide/classifier/SoftmaxRegression/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络\n",
    "\n",
    "### 优化器（Optimizer）\n",
    "参考：\n",
    "> http://ruder.io/optimizing-gradient-descent/index.html#rmsprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Resources from reviewer\n",
    "\n",
    "- `conda`环境管理：https://conda.io/docs/user-guide/tasks/manage-environments.html\n",
    "- Python3引入了类型检查：https://docs.python.org/3/library/typing.html\n",
    "- 使用`numpy`来计算词频：\n",
    " ```python\n",
    "import numpy as np\n",
    "x = np.array([1,1,1,2,2,2,5,25,1,1])\n",
    "unique, counts = np.unique(x, return_counts=True)\n",
    "print np.asarray((unique, counts)).T\n",
    "```\n",
    "参考：https://stackoverflow.com/questions/10741346/numpy-most-efficient-frequency-counts-for-unique-values-in-an-array\n",
    "- nltk中包含了很多种词干提取器，包括PorterStemmer, SnowballStemmer, Arabic Stemmer, LancasterStemmer.\n",
    "你可以在[这里](https://www.nltk.org/api/nltk.stem.html)找到更多资料。\n",
    "- `numpy`上常用的统计函数：https://docs.scipy.org/doc/numpy/reference/routines.statistics.html\n",
    "- 使用$R^2$评价模型表现，及其局限性：https://en.wikipedia.org/wiki/Coefficient_of_determination#Caveats\n",
    "- `sklearn`中常用的模型评价工具：http://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "- 关于学习曲线，传统的机器学习算法（又被称为基于统计的机器学习）在数据量达到一定程度后，更多的数据无法提升模型的表现。深度学习的一个优势就是它可以把大量的数据利用起来，提升学习表现。\n",
    "<img src=\"./images/ml_curve.png\" width=\"360px\"/>\n",
    "\n",
    " 参考：\n",
    " - https://www.coursera.org/learn/machine-learning/lecture/Kont7/learning-curves\n",
    " - http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
    "- Bias and Variance Tradeoff：\n",
    "<img src=\"./images/bias-variance-tradeoff.jpeg\" width=\"360px\"/>\n",
    "\n",
    " - «机器学习», 周志华, 2.5 节偏差与方差.\n",
    " - http://scott.fortmann-roe.com/docs/BiasVariance.html\n",
    "- Learning from examples 是掌握一个算法模型的关键，这个[sklearn gallery](http://scikit-learn.org/stable/auto_examples/index.html)是一个非常好的资源：\n",
    "- 判断一个模型是否适合该问题，可以从数据规模，问题类型，模型复杂度等等来谈.这个[sklearn](http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)的算法地图能够帮你快速导航到相关的算法模型。\n",
    "- 同时，微软这两个机器学习算法模型备忘录也是份很不错的笔记：\n",
    " - [cheat sheet1, considerations when choosing an algorithm](https://docs.microsoft.com/en-us/azure/machine-learning/studio/algorithm-choice#considerations-when-choosing-an-algorithm)\n",
    " - [algorithm cheat sheet](https://docs.microsoft.com/en-us/azure/machine-learning/studio/algorithm-cheat-sheet#download-machine-learning-algorithm-cheat-sheet)\n",
    "- `Adaboost`，你可以从这篇论文 http://cseweb.ucsd.edu/~yfrefund/papers/IntroToBoosting.pdf 了解到更多有关`Adaboost`的训练细节，你可以仔细研读论文中`Adaboost`这一小节，其他篇章可以泛读。大概需要15~20分钟的时间你就可以对`Adaboost`有更深的认识。\n",
    "- **随机调参**，`Adaboost`的调参过程很耗费时间，所以使用[随机调参](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)可以节省由遍历搜索所耗费的大量时间，同时，最终调参得到的模型性能还是有保证的。\n",
    "<img src=\"./images/random-params.png\" width=\"350px\"/>\n",
    "\n",
    " 在`sklearn`中使用：\n",
    " ```python\n",
    "clf = ... # 模型\n",
    "param_dist = ... # 参数列表\n",
    "n_iter_search = ... # 你想要搜索的次数\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search)\n",
    "random_search.fit(X, y)\n",
    " ```\n",
    " 你可以从这里阅读到`gridsearch`与`RandomSearch`两者的比较。你也可以从这篇[通俗易懂的论文](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)了解到更多相关的知识。\n",
    "- 特征选择：http://www.cnblogs.com/heaad/archive/2011/01/02/1924088.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
